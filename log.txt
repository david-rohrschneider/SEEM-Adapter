nohup: ignoring input
WARNING:utils.arguments:Overrided MODEL.ENCODER.NUM_CLASSES from 4 to 1
WARNING:utils.arguments:Overrided TEST.BATCH_SIZE_TOTAL from 8 to 4
WARNING:utils.arguments:Overrided TRAIN.BATCH_SIZE_TOTAL from 4 to 4
WARNING:utils.arguments:Overrided TRAIN.BATCH_SIZE_PER_GPU from 4 to 2
WARNING:utils.arguments:Overrided DATASETS.TRAIN from ['dolphin-train'] to ['dolphin-train']
WARNING:utils.arguments:Overrided DATASETS.TEST from ['dolphin-val'] to ['dolphin-val']
WARNING:utils.arguments:Overrided INPUT.PIXEL_MEAN from [92.2797, 154.551, 153.192] to [92.2797, 154.551, 153.192]
WARNING:utils.arguments:Overrided INPUT.PIXEL_STD from [67.1433, 39.6286, 41.3266] to [67.1433, 39.6286, 41.3266]
WARNING:utils.arguments:Overrided SOLVER.MAX_NUM_EPOCHS from 50 to 15
WARNING:utils.arguments:Overrided SOLVER.BASE_LR from 0.001 to 0.001
WARNING:utils.arguments:Overrided SOLVER.STEPS from [0.88889, 0.96296] to [0.88889, 0.96296]
WARNING:utils.arguments:Overrided SOLVER.IGNORE_FIX from ['class_embed', 'mask_embed'] to ['class_embed', 'mask_embed']
WARNING:utils.arguments:Overrided LORA_TARGETS from ['q', 'v'] to ['q', 'v']
WARNING:utils.arguments:Overrided LORA_RANK from 8 to 8
WARNING:utils.arguments:Overrided LORA_ALPHA from 8 to 8
WARNING:utils.arguments:Overrided WANDB_EXP_NAME from SEEM-Adapter to ndd20_lora_alpha-8
WARNING:utils.arguments:Overrided MODEL.ENCODER.NUM_CLASSES from 4 to 1
WARNING:utils.arguments:Overrided TEST.BATCH_SIZE_TOTAL from 8 to 4
WARNING:utils.arguments:Overrided TRAIN.BATCH_SIZE_TOTAL from 4 to 4
WARNING:utils.arguments:Overrided TRAIN.BATCH_SIZE_PER_GPU from 4 to 2
WARNING:utils.arguments:Overrided DATASETS.TRAIN from ['dolphin-train'] to ['dolphin-train']
WARNING:utils.arguments:Overrided DATASETS.TEST from ['dolphin-val'] to ['dolphin-val']
WARNING:utils.arguments:Overrided INPUT.PIXEL_MEAN from [92.2797, 154.551, 153.192] to [92.2797, 154.551, 153.192]
WARNING:utils.arguments:Overrided INPUT.PIXEL_STD from [67.1433, 39.6286, 41.3266] to [67.1433, 39.6286, 41.3266]
WARNING:utils.arguments:Overrided SOLVER.MAX_NUM_EPOCHS from 50 to 15
WARNING:utils.arguments:Overrided SOLVER.BASE_LR from 0.001 to 0.001
WARNING:utils.arguments:Overrided SOLVER.STEPS from [0.88889, 0.96296] to [0.88889, 0.96296]
WARNING:utils.arguments:Overrided SOLVER.IGNORE_FIX from ['class_embed', 'mask_embed'] to ['class_embed', 'mask_embed']
WARNING:utils.arguments:Overrided LORA_TARGETS from ['q', 'v'] to ['q', 'v']
WARNING:utils.arguments:Overrided LORA_RANK from 8 to 8
WARNING:utils.arguments:Overrided LORA_ALPHA from 8 to 8
WARNING:utils.arguments:Overrided WANDB_EXP_NAME from SEEM-Adapter to ndd20_lora_alpha-8
INFO:trainer.distributed_trainer:Setting SAVE_DIR as output
INFO:trainer.distributed_trainer:Setting SAVE_DIR as output
INFO:trainer.distributed_trainer:Using CUDA
WARNING:trainer.utils.mpi_adapter:----------------
WARNING:trainer.utils.mpi_adapter:MPI Adapter data
WARNING:trainer.utils.mpi_adapter:----------------
WARNING:trainer.utils.mpi_adapter:environment info: single-node AML or other MPI environment
WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:36873
WARNING:trainer.utils.mpi_adapter:world size: 2
WARNING:trainer.utils.mpi_adapter:local size: 2
WARNING:trainer.utils.mpi_adapter:rank: 0
WARNING:trainer.utils.mpi_adapter:local rank: 0
WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1
WARNING:trainer.utils.mpi_adapter:master port: 36873
WARNING:trainer.utils.mpi_adapter:----------------
WARNING:trainer.utils.mpi_adapter:trying to initialize process group ...
INFO:trainer.distributed_trainer:Using CUDA
WARNING:trainer.utils.mpi_adapter:----------------
WARNING:trainer.utils.mpi_adapter:MPI Adapter data
WARNING:trainer.utils.mpi_adapter:----------------
WARNING:trainer.utils.mpi_adapter:environment info: single-node AML or other MPI environment
WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:36873
WARNING:trainer.utils.mpi_adapter:world size: 2
WARNING:trainer.utils.mpi_adapter:local size: 2
WARNING:trainer.utils.mpi_adapter:rank: 1
WARNING:trainer.utils.mpi_adapter:local rank: 1
WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1
WARNING:trainer.utils.mpi_adapter:master port: 36873
WARNING:trainer.utils.mpi_adapter:----------------
WARNING:trainer.utils.mpi_adapter:trying to initialize process group ...
WARNING:trainer.utils.mpi_adapter:process group initialized
WARNING:trainer.utils.mpi_adapter:process group initialized
INFO:trainer.distributed_trainer:Base learning rate: 0.001
INFO:trainer.distributed_trainer:Number of GPUs: 2
INFO:trainer.distributed_trainer:Gradient accumulation steps: 1
INFO:trainer.utils.hook:Adding global except hook for the distributed job to shutdown MPI if unhandled exception is raised on some of the ranks.
INFO:trainer.distributed_trainer:Save config file to output/conf_copy.yaml
INFO:trainer.distributed_trainer:Base learning rate: 0.001
INFO:trainer.distributed_trainer:Number of GPUs: 2
INFO:trainer.distributed_trainer:Gradient accumulation steps: 1
INFO:trainer.utils.hook:Adding global except hook for the distributed job to shutdown MPI if unhandled exception is raised on some of the ranks.
INFO:trainer.default_trainer:Imported base_dir at base_path ./
INFO:trainer.default_trainer:Imported base_dir at base_path ./
Deformable Transformer Encoder is not available.
Deformable Transformer Encoder is not available.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.
INFO:trainer.default_trainer:Pipeline for training: XDecoderPipeline
INFO:trainer.default_trainer:-------------------------------------------------------
INFO:trainer.default_trainer:Training on rank: 1
INFO:trainer.default_trainer:Pipeline for training: XDecoderPipeline
wandb: Currently logged in as: david-rohrschneider. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/hrw/.netrc
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in output/focall_unicl_lang_v1.yaml_conf~/run_6/wandb/wandb/run-20240915_192618-fjlrnf58
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ndd20_lora_alpha-8__run_6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/david-rohrschneider/SEEM-adapter
wandb: üöÄ View run at https://wandb.ai/david-rohrschneider/SEEM-adapter/runs/fjlrnf58
INFO:trainer.default_trainer:-------------------------------------------------------
INFO:trainer.default_trainer:Training on rank: 0
INFO:datasets.dataset_mappers.dolphin_dataset_mapper:[Dolphin DatasetMapper] Full TransformGens used: [RandomFlip(), ResizeScale(min_scale=0.9, max_scale=2.0, target_height=512, target_width=512), Resize(shape=(512, 512))]
INFO:detectron2.data.datasets.coco:Loaded 1500 images in COCO format from /home/hrw/datasets/ndd20/coco/train_1500.coco.json
INFO:datasets.build:Removed 0 images with no usable annotations. 1500 images left.
INFO:datasets.build:Using training sampler TrainingSampler
INFO:base_dir.pipeline.XDecoderPipeline:GeneralizedSEEM(
  (backbone): D2FocalNet(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 192, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        (blocks): ModuleList(
          (0): FocalModulationBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (modulation): FocalModulation(
              (f): Linear(in_features=192, out_features=389, bias=True)
              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (focal_layers): ModuleList(
                (0): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
                (1): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
                (2): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
                (3): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
              )
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): FocalModulationBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (modulation): FocalModulation(
              (f): Linear(in_features=192, out_features=389, bias=True)
              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (focal_layers): ModuleList(
                (0): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
                (1): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
                (2): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
                (3): Sequential(
                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)
                  (1): GELU(approximate='none')
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchEmbed(
          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BasicLayer(
        (blocks): ModuleList(
          (0-1): 2 x FocalModulationBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (modulation): FocalModulation(
              (f): Linear(in_features=384, out_features=773, bias=True)
              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (focal_layers): ModuleList(
                (0): Sequential(
                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                  (1): GELU(approximate='none')
                )
                (1): Sequential(
                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                  (1): GELU(approximate='none')
                )
                (2): Sequential(
                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                  (1): GELU(approximate='none')
                )
                (3): Sequential(
                  (0): Conv2d(384, 384, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=384, bias=False)
                  (1): GELU(approximate='none')
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchEmbed(
          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        (blocks): ModuleList(
          (0-17): 18 x FocalModulationBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (modulation): FocalModulation(
              (f): Linear(in_features=768, out_features=1541, bias=True)
              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (focal_layers): ModuleList(
                (0): Sequential(
                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                  (1): GELU(approximate='none')
                )
                (1): Sequential(
                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                  (1): GELU(approximate='none')
                )
                (2): Sequential(
                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)
                  (1): GELU(approximate='none')
                )
                (3): Sequential(
                  (0): Conv2d(768, 768, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=768, bias=False)
                  (1): GELU(approximate='none')
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchEmbed(
          (proj): Conv2d(768, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        (blocks): ModuleList(
          (0-1): 2 x FocalModulationBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (modulation): FocalModulation(
              (f): Linear(in_features=1536, out_features=3077, bias=True)
              (h): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (focal_layers): ModuleList(
                (0): Sequential(
                  (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
                  (1): GELU(approximate='none')
                )
                (1): Sequential(
                  (0): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1536, bias=False)
                  (1): GELU(approximate='none')
                )
                (2): Sequential(
                  (0): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536, bias=False)
                  (1): GELU(approximate='none')
                )
                (3): Sequential(
                  (0): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=1536, bias=False)
                  (1): GELU(approximate='none')
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (sem_seg_head): XdecoderHead(
    (pixel_decoder): TransformerEncoderPixelDecoder(
      (adapter_1): Conv2d(
        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (input_proj): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
      (transformer): TransformerEncoderOnly(
        (encoder): TransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 256
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (layer_4): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
      )
    )
    (predictor): SEEMDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 256
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(101, 512)
      (query_embed): Embedding(101, 512)
      (level_embed): Embedding(3, 512)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (lang_encoder): LanguageEncoder(
        (lang_encoder): Transformer(
          (token_embedding): Embedding(49408, 512)
          (resblocks): ModuleList(
            (0-11): 12 x ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm()
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm()
              (drop_path): Identity()
            )
          )
          (ln_final): LayerNorm()
        )
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)
        )
      )
      (attention_data): AttentionDataStruct()
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: []
      weight_dict: {'loss_mask_ce_0': 2.0, 'loss_mask_dice_0': 5.0, 'loss_mask_bce_0': 5.0, 'loss_openimage_ce_0': 0.4, 'loss_openimage_dice_0': 1.0, 'loss_openimage_bce_0': 1.0, 'loss_mask_ce_1': 2.0, 'loss_mask_dice_1': 5.0, 'loss_mask_bce_1': 5.0, 'loss_openimage_ce_1': 0.4, 'loss_openimage_dice_1': 1.0, 'loss_openimage_bce_1': 1.0, 'loss_mask_ce_2': 2.0, 'loss_mask_dice_2': 5.0, 'loss_mask_bce_2': 5.0, 'loss_openimage_ce_2': 0.4, 'loss_openimage_dice_2': 1.0, 'loss_openimage_bce_2': 1.0, 'loss_mask_ce_3': 2.0, 'loss_mask_dice_3': 5.0, 'loss_mask_bce_3': 5.0, 'loss_openimage_ce_3': 0.4, 'loss_openimage_dice_3': 1.0, 'loss_openimage_bce_3': 1.0, 'loss_mask_ce_4': 2.0, 'loss_mask_dice_4': 5.0, 'loss_mask_bce_4': 5.0, 'loss_openimage_ce_4': 0.4, 'loss_openimage_dice_4': 1.0, 'loss_openimage_bce_4': 1.0, 'loss_mask_ce_5': 2.0, 'loss_mask_dice_5': 5.0, 'loss_mask_bce_5': 5.0, 'loss_openimage_ce_5': 0.4, 'loss_openimage_dice_5': 1.0, 'loss_openimage_bce_5': 1.0, 'loss_mask_ce_6': 2.0, 'loss_mask_dice_6': 5.0, 'loss_mask_bce_6': 5.0, 'loss_openimage_ce_6': 0.4, 'loss_openimage_dice_6': 1.0, 'loss_openimage_bce_6': 1.0, 'loss_mask_ce_7': 2.0, 'loss_mask_dice_7': 5.0, 'loss_mask_bce_7': 5.0, 'loss_openimage_ce_7': 0.4, 'loss_openimage_dice_7': 1.0, 'loss_openimage_bce_7': 1.0, 'loss_mask_ce_8': 2.0, 'loss_mask_dice_8': 5.0, 'loss_mask_bce_8': 5.0, 'loss_openimage_ce_8': 0.4, 'loss_openimage_dice_8': 1.0, 'loss_openimage_bce_8': 1.0, 'loss_mask_ce_9': 2.0, 'loss_mask_dice_9': 5.0, 'loss_mask_bce_9': 5.0, 'loss_openimage_ce_9': 0.4, 'loss_openimage_dice_9': 1.0, 'loss_openimage_bce_9': 1.0}
      num_classes: 1
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
INFO:datasets.dataset_mappers.dolphin_dataset_mapper:[Dolphin DatasetMapper] Full TransformGens used: [RandomFlip(), ResizeScale(min_scale=0.9, max_scale=2.0, target_height=512, target_width=512), Resize(shape=(512, 512))]
INFO:detectron2.data.datasets.coco:Loaded 1500 images in COCO format from /home/hrw/datasets/ndd20/coco/train_1500.coco.json
INFO:datasets.build:Removed 0 images with no usable annotations. 1500 images left.
INFO:datasets.build:Using training sampler TrainingSampler
INFO:detectron2.data.common:Serializing 1500 elements to byte tensors and concatenating them all ...
INFO:detectron2.data.common:Serializing 1500 elements to byte tensors and concatenating them all ...
INFO:detectron2.data.common:Serialized dataset takes 1.72 MiB
INFO:detectron2.data.common:Serialized dataset takes 1.72 MiB
INFO:base_dir.pipeline.XDecoderPipeline:num of train samples: 375
INFO:base_dir.pipeline.XDecoderPipeline:num of train samples: 375
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.class_embed: 0.1
WARNING:trainer.utils_trainer:PyTorch AMP GradScaler initialized.
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.mask_embed.layers.0.weight: 0.1
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.mask_embed.layers.0.bias: 0.1
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.mask_embed.layers.1.weight: 0.1
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.mask_embed.layers.1.bias: 0.1
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.mask_embed.layers.2.weight: 0.1
INFO:trainer.xdecoder_trainer:Modify Learning rate of model.sem_seg_head.predictor.mask_embed.layers.2.bias: 0.1
INFO:trainer.xdecoder_trainer:TRAINABLE PARAMS SUMMARY:
Total parameters: 340754169
Sum of trained parameters: 1443328 (0.42%)
Sum of frozen parameters: 339310841 (99.58%)
+------------+----------------------------------------------------------------------------------+-------------+--------------+
| Model name |                                   Module name                                    |  Param Name | # Parameters |
+------------+----------------------------------------------------------------------------------+-------------+--------------+
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn      |  qv_lora_A  |     4096     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn      |  qv_lora_B  |    12288     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn      |  qv_lora_A  |     4096     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn      |  qv_lora_B  |    12288     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn      |  qv_lora_A  |     4096     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn      |  qv_lora_B  |    12288     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn      |  qv_lora_A  |     4096     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn      |  qv_lora_B  |    12288     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn      |  qv_lora_A  |     4096     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn      |  qv_lora_B  |    12288     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn      |  qv_lora_A  |     4096     |
|  default   |     model.sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn      |  qv_lora_B  |    12288     |
|  default   |                           model.sem_seg_head.predictor                           | class_embed |    262144    |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn    |  qv_lora_B  |    12288     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn    |  qv_lora_A  |     4096     |
|  default   |    model.sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn    |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn |  qv_lora_B  |    12288     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn |  qv_lora_A  |     4096     |
|  default   | model.sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn |  qv_lora_B  |    12288     |
|  default   |                 model.sem_seg_head.predictor.mask_embed.layers.0                 |    weight   |    262144    |
|  default   |                 model.sem_seg_head.predictor.mask_embed.layers.0                 |     bias    |     512      |
|  default   |                 model.sem_seg_head.predictor.mask_embed.layers.1                 |    weight   |    262144    |
|  default   |                 model.sem_seg_head.predictor.mask_embed.layers.1                 |     bias    |     512      |
|  default   |                 model.sem_seg_head.predictor.mask_embed.layers.2                 |    weight   |    262144    |
|  default   |                 model.sem_seg_head.predictor.mask_embed.layers.2                 |     bias    |     512      |
+------------+----------------------------------------------------------------------------------+-------------+--------------+
INFO:trainer.xdecoder_trainer:TOTAL MODEL SIZE (MB): 1299.874 MB
INFO:trainer.xdecoder_trainer:Calculate MAX_ITER @ 5625 and STEPS @ [5000, 5416]
WARNING:trainer.utils_trainer:PyTorch AMP GradScaler initialized.
INFO:trainer.default_trainer:Loading weights from seem_focall_v1.pt
INFO:trainer.default_trainer:Loading weights from seem_focall_v1.pt
WARNING:utils.model:$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])
WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_sptial_embed.0, Ckpt Shape: torch.Size([512, 512])
WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_sptial_embed.1, Ckpt Shape: torch.Size([512, 512])
WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_sptial_embed.2, Ckpt Shape: torch.Size([512, 512])
WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.pn_indicator.weight, Ckpt Shape: torch.Size([2, 512])
WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.spatial_embed.weight, Ckpt Shape: torch.Size([32, 512])
WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.spatial_featured.weight, Ckpt Shape: torch.Size([32, 512])
WARNING:utils.model:*UNMATCHED* criterion.empty_weight, Model Shape: torch.Size([2]) <-> Ckpt Shape: torch.Size([134])
WARNING:trainer.utils_trainer:Load weights from seem_focall_v1.pt...
INFO:trainer.default_trainer:Start epoch: 0 training.
WARNING:trainer.utils_trainer:Load weights from seem_focall_v1.pt...
INFO:trainer.default_trainer:***** Running training *****
INFO:trainer.default_trainer:  Num of GPUs = 2
INFO:trainer.default_trainer:  Num Epochs = 15
INFO:trainer.default_trainer:  Num of Mini Batches per Epoch = 375
INFO:trainer.default_trainer:  Total train batch size (w. parallel, distributed & accumulation) = 5625
INFO:trainer.default_trainer:  Gradient Accumulation steps = 1
INFO:trainer.default_trainer:  Total optimization steps = 5625
INFO:trainer.default_trainer:Start epoch: 0 training.
INFO:trainer.default_trainer:epochs[     0] optim steps[1] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.10517/0.10517, loss_mask_bce_0: 0.11122/0.11122, loss_mask_dice_0: 0.63130/0.63130, loss_mask_ce_1: 0.12680/0.12680, loss_mask_bce_1: 0.10414/0.10414, loss_mask_dice_1: 0.53660/0.53660, loss_mask_ce_2: 0.13376/0.13376, loss_mask_bce_2: 0.11684/0.11684, loss_mask_dice_2: 0.67742/0.67742, loss_mask_ce_3: 0.04016/0.04016, loss_mask_bce_3: 0.10408/0.10408, loss_mask_dice_3: 0.56467/0.56467, loss_mask_ce_4: 0.06441/0.06441, loss_mask_bce_4: 0.11772/0.11772, loss_mask_dice_4: 0.59594/0.59594, loss_mask_ce_5: 0.52296/0.52296, loss_mask_bce_5: 0.11590/0.11590, loss_mask_dice_5: 0.62119/0.62119, loss_mask_ce_6: 0.16965/0.16965, loss_mask_bce_6: 0.11180/0.11180, loss_mask_dice_6: 0.60412/0.60412, loss_mask_ce_7: 0.49492/0.49492, loss_mask_bce_7: 0.32693/0.32693, loss_mask_dice_7: 1.12268/1.12268, loss_mask_ce_8: 2.10149/2.10149, loss_mask_bce_8: 0.11872/0.11872, loss_mask_dice_8: 0.69802/0.69802, loss_mask_ce_9: 0.86857/0.86857, loss_mask_bce_9: 0.17638/0.17638, loss_mask_dice_9: 1.12140/1.12140] items per batch[4] items per second[2.78] total items[4] mini batches[     1] memory[2619] epoch remaining[0:08:58]
INFO:trainer.default_trainer:epochs[     0] optim steps[2] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.21359/0.15938, loss_mask_bce_0: 0.01836/0.06479, loss_mask_dice_0: 0.45305/0.54218, loss_mask_ce_1: 0.23538/0.18109, loss_mask_bce_1: 0.02135/0.06274, loss_mask_dice_1: 0.50857/0.52258, loss_mask_ce_2: 0.21826/0.17601, loss_mask_bce_2: 0.02113/0.06898, loss_mask_dice_2: 0.52714/0.60228, loss_mask_ce_3: 0.14670/0.09343, loss_mask_bce_3: 0.01838/0.06123, loss_mask_dice_3: 0.46789/0.51628, loss_mask_ce_4: 0.17098/0.11769, loss_mask_bce_4: 0.01918/0.06845, loss_mask_dice_4: 0.56790/0.58192, loss_mask_ce_5: 0.52902/0.52599, loss_mask_bce_5: 0.02178/0.06884, loss_mask_dice_5: 0.65128/0.63623, loss_mask_ce_6: 0.18779/0.17872, loss_mask_bce_6: 0.01867/0.06523, loss_mask_dice_6: 0.56229/0.58321, loss_mask_ce_7: 1.03076/0.76284, loss_mask_bce_7: 0.01828/0.17260, loss_mask_dice_7: 0.50586/0.81427, loss_mask_ce_8: 1.33272/1.71711, loss_mask_bce_8: 0.03572/0.07722, loss_mask_dice_8: 0.81960/0.75881, loss_mask_ce_9: 0.89169/0.88013, loss_mask_bce_9: 0.03637/0.10637, loss_mask_dice_9: 1.15123/1.13632] items per batch[4] items per second[7.89] total items[8] mini batches[     2] memory[2619] epoch remaining[0:06:03]
INFO:trainer.default_trainer:epochs[     0] optim steps[3] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.90229/0.40701, loss_mask_bce_0: 0.13517/0.08825, loss_mask_dice_0: 1.23648/0.77361, loss_mask_ce_1: 0.77819/0.38013, loss_mask_bce_1: 0.19796/0.10782, loss_mask_dice_1: 1.46613/0.83710, loss_mask_ce_2: 0.96452/0.43884, loss_mask_bce_2: 0.14300/0.09366, loss_mask_dice_2: 1.26411/0.82289, loss_mask_ce_3: 0.55242/0.24643, loss_mask_bce_3: 0.19187/0.10477, loss_mask_dice_3: 1.36748/0.80001, loss_mask_ce_4: 0.65988/0.29842, loss_mask_bce_4: 0.22062/0.11917, loss_mask_dice_4: 1.74340/0.96908, loss_mask_ce_5: 0.64058/0.56419, loss_mask_bce_5: 0.22863/0.12210, loss_mask_dice_5: 1.79705/1.02317, loss_mask_ce_6: 0.44364/0.26702, loss_mask_bce_6: 0.22394/0.11814, loss_mask_dice_6: 1.64218/0.93620, loss_mask_ce_7: 1.58136/1.03568, loss_mask_bce_7: 0.15738/0.16753, loss_mask_dice_7: 1.47876/1.03576, loss_mask_ce_8: 2.22358/1.88593, loss_mask_bce_8: 0.15851/0.10431, loss_mask_dice_8: 1.64160/1.05307, loss_mask_ce_9: 1.27088/1.01038, loss_mask_bce_9: 0.15164/0.12146, loss_mask_dice_9: 2.02679/1.43314] items per batch[4] items per second[7.48] total items[12] mini batches[     3] memory[2619] epoch remaining[0:05:07]
INFO:trainer.default_trainer:epochs[     0] optim steps[4] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.71153/0.48314, loss_mask_bce_0: 0.01776/0.07063, loss_mask_dice_0: 1.14755/0.86710, loss_mask_ce_1: 0.46349/0.40097, loss_mask_bce_1: 0.01156/0.08375, loss_mask_dice_1: 0.90023/0.85288, loss_mask_ce_2: 0.76093/0.51936, loss_mask_bce_2: 0.00624/0.07180, loss_mask_dice_2: 1.08464/0.88833, loss_mask_ce_3: 0.70080/0.36002, loss_mask_bce_3: 0.00870/0.08076, loss_mask_dice_3: 1.33478/0.93370, loss_mask_ce_4: 0.65735/0.38815, loss_mask_bce_4: 0.00885/0.09159, loss_mask_dice_4: 0.84007/0.93683, loss_mask_ce_5: 0.81035/0.62573, loss_mask_bce_5: 0.00916/0.09387, loss_mask_dice_5: 1.04092/1.02761, loss_mask_ce_6: 0.22480/0.25647, loss_mask_bce_6: 0.01142/0.09146, loss_mask_dice_6: 1.18597/0.99864, loss_mask_ce_7: 0.23155/0.83465, loss_mask_bce_7: 0.00765/0.12756, loss_mask_dice_7: 1.00156/1.02721, loss_mask_ce_8: 1.65824/1.82901, loss_mask_bce_8: 0.00748/0.08010, loss_mask_dice_8: 1.11548/1.06867, loss_mask_ce_9: 0.97110/1.00056, loss_mask_bce_9: 0.01504/0.09486, loss_mask_dice_9: 2.33259/1.65800] items per batch[4] items per second[26.38] total items[16] mini batches[     4] memory[2619] epoch remaining[0:04:04]
INFO:trainer.default_trainer:epochs[     0] optim steps[5] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.18054/0.42262, loss_mask_bce_0: 0.16219/0.08894, loss_mask_dice_0: 0.62104/0.81789, loss_mask_ce_1: 0.22570/0.36591, loss_mask_bce_1: 0.15203/0.09741, loss_mask_dice_1: 0.58017/0.79834, loss_mask_ce_2: 0.26018/0.46753, loss_mask_bce_2: 0.15041/0.08752, loss_mask_dice_2: 0.61190/0.83304, loss_mask_ce_3: 0.10846/0.30971, loss_mask_bce_3: 0.14834/0.09427, loss_mask_dice_3: 0.50538/0.84804, loss_mask_ce_4: 0.17848/0.34622, loss_mask_bce_4: 0.19628/0.11253, loss_mask_dice_4: 0.68555/0.88657, loss_mask_ce_5: 0.25576/0.55173, loss_mask_bce_5: 0.21843/0.11878, loss_mask_dice_5: 0.94600/1.01129, loss_mask_ce_6: 0.09840/0.22485, loss_mask_bce_6: 0.21897/0.11696, loss_mask_dice_6: 0.64248/0.92741, loss_mask_ce_7: 0.55132/0.77798, loss_mask_bce_7: 0.14620/0.13129, loss_mask_dice_7: 0.55992/0.93375, loss_mask_ce_8: 1.79964/1.82313, loss_mask_bce_8: 0.16388/0.09686, loss_mask_dice_8: 1.05751/1.06644, loss_mask_ce_9: 0.91263/0.98297, loss_mask_bce_9: 0.26539/0.12896, loss_mask_dice_9: 1.43537/1.61347] items per batch[4] items per second[21.53] total items[20] mini batches[     5] memory[2619] epoch remaining[0:03:28]
INFO:trainer.default_trainer:epochs[     0] optim steps[6] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.11999/0.37218, loss_mask_bce_0: 0.14499/0.09828, loss_mask_dice_0: 0.68780/0.79620, loss_mask_ce_1: 0.20778/0.33956, loss_mask_bce_1: 0.15618/0.10720, loss_mask_dice_1: 0.80988/0.80026, loss_mask_ce_2: 0.12237/0.41000, loss_mask_bce_2: 0.15738/0.09917, loss_mask_dice_2: 0.76839/0.82227, loss_mask_ce_3: 0.15105/0.28326, loss_mask_bce_3: 0.18317/0.10909, loss_mask_dice_3: 0.85017/0.84839, loss_mask_ce_4: 0.15614/0.31454, loss_mask_bce_4: 0.19004/0.12545, loss_mask_dice_4: 0.86923/0.88368, loss_mask_ce_5: 0.13263/0.48188, loss_mask_bce_5: 0.17199/0.12765, loss_mask_dice_5: 0.94350/0.99999, loss_mask_ce_6: 0.21451/0.22313, loss_mask_bce_6: 0.18434/0.12819, loss_mask_dice_6: 0.90163/0.92311, loss_mask_ce_7: 0.10399/0.66565, loss_mask_bce_7: 0.32734/0.16396, loss_mask_dice_7: 1.45222/1.02017, loss_mask_ce_8: 2.56896/1.94744, loss_mask_bce_8: 0.17600/0.11005, loss_mask_dice_8: 0.78397/1.01936, loss_mask_ce_9: 1.19717/1.01867, loss_mask_bce_9: 0.25671/0.15025, loss_mask_dice_9: 1.76869/1.63934] items per batch[4] items per second[25.50] total items[24] mini batches[     6] memory[2619] epoch remaining[0:03:03]
INFO:trainer.default_trainer:epochs[     0] optim steps[7] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 1.27286/0.50085, loss_mask_bce_0: 0.61946/0.17274, loss_mask_dice_0: 1.13057/0.84397, loss_mask_ce_1: 0.85507/0.41320, loss_mask_bce_1: 0.60431/0.17822, loss_mask_dice_1: 1.84132/0.94899, loss_mask_ce_2: 0.92584/0.48369, loss_mask_bce_2: 0.56881/0.16626, loss_mask_dice_2: 1.54236/0.92514, loss_mask_ce_3: 0.47248/0.31030, loss_mask_bce_3: 0.57290/0.17535, loss_mask_dice_3: 1.81806/0.98692, loss_mask_ce_4: 0.71641/0.37195, loss_mask_bce_4: 0.62817/0.19727, loss_mask_dice_4: 2.27265/1.08211, loss_mask_ce_5: 0.71095/0.51461, loss_mask_bce_5: 0.52161/0.18393, loss_mask_dice_5: 1.13853/1.01978, loss_mask_ce_6: 0.91627/0.32215, loss_mask_bce_6: 0.51053/0.18281, loss_mask_dice_6: 1.05875/0.94249, loss_mask_ce_7: 0.70414/0.67115, loss_mask_bce_7: 0.72862/0.24463, loss_mask_dice_7: 1.72228/1.12047, loss_mask_ce_8: 1.51179/1.88520, loss_mask_bce_8: 0.40701/0.15247, loss_mask_dice_8: 1.71905/1.11932, loss_mask_ce_9: 1.31849/1.06150, loss_mask_bce_9: 0.39636/0.18541, loss_mask_dice_9: 3.01559/1.83595] items per batch[4] items per second[25.93] total items[28] mini batches[     7] memory[2619] epoch remaining[0:02:44]
INFO:trainer.default_trainer:epochs[     0] optim steps[8] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.06233/0.44604, loss_mask_bce_0: 0.17978/0.17362, loss_mask_dice_0: 0.28561/0.77418, loss_mask_ce_1: 0.05521/0.36845, loss_mask_bce_1: 0.18134/0.17861, loss_mask_dice_1: 0.26002/0.86287, loss_mask_ce_2: 0.05945/0.43066, loss_mask_bce_2: 0.18945/0.16916, loss_mask_dice_2: 0.28971/0.84571, loss_mask_ce_3: 0.08540/0.28218, loss_mask_bce_3: 0.18666/0.17676, loss_mask_dice_3: 0.28516/0.89920, loss_mask_ce_4: 0.08455/0.33602, loss_mask_bce_4: 0.18843/0.19616, loss_mask_dice_4: 0.28165/0.98205, loss_mask_ce_5: 0.12529/0.46594, loss_mask_bce_5: 0.18372/0.18390, loss_mask_dice_5: 0.29034/0.92860, loss_mask_ce_6: 0.28838/0.31793, loss_mask_bce_6: 0.19395/0.18420, loss_mask_dice_6: 0.30433/0.86272, loss_mask_ce_7: 0.20134/0.61242, loss_mask_bce_7: 0.22047/0.24161, loss_mask_dice_7: 0.32879/1.02151, loss_mask_ce_8: 0.46413/1.70757, loss_mask_bce_8: 0.21258/0.15999, loss_mask_dice_8: 0.33444/1.02121, loss_mask_ce_9: 1.15255/1.07288, loss_mask_bce_9: 0.34459/0.20531, loss_mask_dice_9: 0.79813/1.70622] items per batch[4] items per second[25.79] total items[32] mini batches[     8] memory[2619] epoch remaining[0:02:30]
INFO:trainer.default_trainer:epochs[     0] optim steps[9] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.13049/0.41098, loss_mask_bce_0: 0.25809/0.18300, loss_mask_dice_0: 0.52419/0.74640, loss_mask_ce_1: 0.10321/0.33898, loss_mask_bce_1: 0.32938/0.19536, loss_mask_dice_1: 0.67667/0.84218, loss_mask_ce_2: 0.09659/0.39354, loss_mask_bce_2: 0.30162/0.18388, loss_mask_dice_2: 0.69726/0.82921, loss_mask_ce_3: 0.14399/0.26683, loss_mask_bce_3: 0.29067/0.18942, loss_mask_dice_3: 0.60042/0.86600, loss_mask_ce_4: 0.11367/0.31132, loss_mask_bce_4: 0.29562/0.20721, loss_mask_dice_4: 0.54623/0.93362, loss_mask_ce_5: 0.15736/0.43166, loss_mask_bce_5: 0.30357/0.19720, loss_mask_dice_5: 0.61662/0.89394, loss_mask_ce_6: 0.21330/0.30630, loss_mask_bce_6: 0.27092/0.19384, loss_mask_dice_6: 0.47579/0.81973, loss_mask_ce_7: 0.09570/0.55501, loss_mask_bce_7: 0.25704/0.24332, loss_mask_dice_7: 0.44225/0.95715, loss_mask_ce_8: 1.57100/1.69239, loss_mask_bce_8: 0.27726/0.17302, loss_mask_dice_8: 0.44120/0.95676, loss_mask_ce_9: 0.99788/1.06455, loss_mask_bce_9: 0.34024/0.22030, loss_mask_dice_9: 0.56661/1.57960] items per batch[4] items per second[25.62] total items[36] mini batches[     9] memory[2619] epoch remaining[0:02:19]
INFO:trainer.default_trainer:epochs[     0] optim steps[10] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.19782/0.38966, loss_mask_bce_0: 0.27305/0.19201, loss_mask_dice_0: 0.23848/0.69561, loss_mask_ce_1: 0.12484/0.31757, loss_mask_bce_1: 0.29217/0.20504, loss_mask_dice_1: 0.25925/0.78388, loss_mask_ce_2: 0.13673/0.36786, loss_mask_bce_2: 0.26504/0.19199, loss_mask_dice_2: 0.23382/0.76967, loss_mask_ce_3: 0.14700/0.25485, loss_mask_bce_3: 0.28160/0.19864, loss_mask_dice_3: 0.25398/0.80480, loss_mask_ce_4: 0.10566/0.29075, loss_mask_bce_4: 0.29140/0.21563, loss_mask_dice_4: 0.28613/0.86887, loss_mask_ce_5: 0.20606/0.40910, loss_mask_bce_5: 0.31581/0.20906, loss_mask_dice_5: 0.37385/0.84193, loss_mask_ce_6: 0.32840/0.30851, loss_mask_bce_6: 0.27149/0.20160, loss_mask_dice_6: 0.29748/0.76750, loss_mask_ce_7: 0.19670/0.51918, loss_mask_bce_7: 0.28327/0.24732, loss_mask_dice_7: 0.31341/0.89277, loss_mask_ce_8: 0.44481/1.56763, loss_mask_bce_8: 0.35846/0.19156, loss_mask_dice_8: 0.67270/0.92836, loss_mask_ce_9: 0.93543/1.05164, loss_mask_bce_9: 0.29868/0.22814, loss_mask_dice_9: 0.61236/1.48288] items per batch[4] items per second[25.81] total items[40] mini batches[    10] memory[2619] epoch remaining[0:02:11]
INFO:trainer.default_trainer:epochs[     0] optim steps[100] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01109/0.07698, loss_mask_bce_0: 0.14035/0.20911, loss_mask_dice_0: 0.66728/0.50004, loss_mask_ce_1: 0.00865/0.06970, loss_mask_bce_1: 0.15124/0.22384, loss_mask_dice_1: 0.53209/0.50868, loss_mask_ce_2: 0.00876/0.07539, loss_mask_bce_2: 0.14498/0.22250, loss_mask_dice_2: 0.74570/0.50746, loss_mask_ce_3: 0.01010/0.06980, loss_mask_bce_3: 0.14410/0.21582, loss_mask_dice_3: 0.94165/0.53777, loss_mask_ce_4: 0.00455/0.07042, loss_mask_bce_4: 0.13159/0.21908, loss_mask_dice_4: 0.54735/0.52749, loss_mask_ce_5: 0.01711/0.08549, loss_mask_bce_5: 0.14735/0.22440, loss_mask_dice_5: 1.11396/0.53481, loss_mask_ce_6: 0.00553/0.07889, loss_mask_bce_6: 0.15044/0.23684, loss_mask_dice_6: 0.91774/0.51593, loss_mask_ce_7: 0.00679/0.12916, loss_mask_bce_7: 0.14390/0.23867, loss_mask_dice_7: 0.70700/0.52898, loss_mask_ce_8: 0.02757/0.25765, loss_mask_bce_8: 0.14277/0.23991, loss_mask_dice_8: 0.76169/0.57634, loss_mask_ce_9: 0.21136/0.32642, loss_mask_bce_9: 0.14797/0.24567, loss_mask_dice_9: 0.79111/0.79755] items per batch[4] items per second[0.28] total items[400] mini batches[   100] memory[2619] epoch remaining[0:00:48]
INFO:trainer.default_trainer:epochs[     0] optim steps[200] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00552/0.06162, loss_mask_bce_0: 0.01276/0.18357, loss_mask_dice_0: 0.27443/0.49803, loss_mask_ce_1: 0.00395/0.06260, loss_mask_bce_1: 0.00976/0.19152, loss_mask_dice_1: 0.21622/0.48752, loss_mask_ce_2: 0.00407/0.05936, loss_mask_bce_2: 0.01399/0.18881, loss_mask_dice_2: 0.28395/0.48820, loss_mask_ce_3: 0.00561/0.05874, loss_mask_bce_3: 0.01765/0.18535, loss_mask_dice_3: 0.29580/0.50423, loss_mask_ce_4: 0.00454/0.05930, loss_mask_bce_4: 0.01634/0.18730, loss_mask_dice_4: 0.30950/0.50508, loss_mask_ce_5: 0.00550/0.06752, loss_mask_bce_5: 0.01320/0.19049, loss_mask_dice_5: 0.23071/0.50811, loss_mask_ce_6: 0.00458/0.06470, loss_mask_bce_6: 0.00738/0.19621, loss_mask_dice_6: 0.17051/0.50128, loss_mask_ce_7: 0.01230/0.09643, loss_mask_bce_7: 0.00919/0.20152, loss_mask_dice_7: 0.21681/0.51443, loss_mask_ce_8: 0.03844/0.17850, loss_mask_bce_8: 0.00892/0.20061, loss_mask_dice_8: 0.22685/0.54054, loss_mask_ce_9: 0.06743/0.23764, loss_mask_bce_9: 0.00813/0.21690, loss_mask_dice_9: 0.34270/0.71207] items per batch[4] items per second[0.26] total items[800] mini batches[   200] memory[2619] epoch remaining[0:00:29]
INFO:trainer.default_trainer:epochs[     0] optim steps[300] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.03555/0.05826, loss_mask_bce_0: 0.25851/0.17601, loss_mask_dice_0: 0.48942/0.51226, loss_mask_ce_1: 0.03310/0.05965, loss_mask_bce_1: 0.27243/0.18146, loss_mask_dice_1: 0.51221/0.51388, loss_mask_ce_2: 0.03241/0.05641, loss_mask_bce_2: 0.27316/0.17921, loss_mask_dice_2: 0.52562/0.50509, loss_mask_ce_3: 0.02012/0.05451, loss_mask_bce_3: 0.33782/0.17804, loss_mask_dice_3: 0.56947/0.51440, loss_mask_ce_4: 0.03369/0.05950, loss_mask_bce_4: 0.33101/0.17911, loss_mask_dice_4: 0.56721/0.52381, loss_mask_ce_5: 0.02133/0.06687, loss_mask_bce_5: 0.30922/0.18107, loss_mask_dice_5: 0.56707/0.52298, loss_mask_ce_6: 0.01208/0.06530, loss_mask_bce_6: 0.33776/0.18565, loss_mask_dice_6: 0.60865/0.51992, loss_mask_ce_7: 0.00462/0.09378, loss_mask_bce_7: 0.47107/0.19036, loss_mask_dice_7: 0.63113/0.53662, loss_mask_ce_8: 0.07204/0.15152, loss_mask_bce_8: 0.54774/0.19156, loss_mask_dice_8: 0.62462/0.55813, loss_mask_ce_9: 0.16009/0.20478, loss_mask_bce_9: 0.20872/0.20960, loss_mask_dice_9: 0.32562/0.71232] items per batch[4] items per second[0.25] total items[1200] mini batches[   300] memory[2619] epoch remaining[0:00:12]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:datasets.dataset_mappers.dolphin_dataset_mapper:[Dolphin DatasetMapper] Full TransformGens used: [Resize(shape=(512, 512))]
INFO:datasets.dataset_mappers.dolphin_dataset_mapper:[Dolphin DatasetMapper] Full TransformGens used: [Resize(shape=(512, 512))]
INFO:detectron2.data.datasets.coco:Loaded 300 images in COCO format from /home/hrw/datasets/ndd20/coco/val_300.coco.json
INFO:detectron2.data.datasets.coco:Loaded 300 images in COCO format from /home/hrw/datasets/ndd20/coco/val_300.coco.json
INFO:detectron2.data.common:Serializing 300 elements to byte tensors and concatenating them all ...
INFO:detectron2.data.common:Serializing 300 elements to byte tensors and concatenating them all ...
INFO:detectron2.data.common:Serialized dataset takes 0.35 MiB
INFO:detectron2.data.common:Serialized dataset takes 0.35 MiB
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0008 s/iter. Inference: 0.1124 s/iter. Eval: 0.3584 s/iter. Total: 0.4716 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0014 s/iter. Inference: 0.1124 s/iter. Eval: 0.3578 s/iter. Total: 0.4716 s/iter. ETA=0:00:24
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0015 s/iter. Inference: 0.1172 s/iter. Eval: 0.3593 s/iter. Total: 0.4781 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1159 s/iter. Eval: 0.3601 s/iter. Total: 0.4776 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1151 s/iter. Eval: 0.3605 s/iter. Total: 0.4773 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0016 s/iter. Inference: 0.1146 s/iter. Eval: 0.3612 s/iter. Total: 0.4775 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:36.662051
INFO:trainer.default_trainer:PROGRESS: 6.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 1 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.16s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.16 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.956
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.851
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.332
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.581
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.833
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.588
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.780
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.803
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.515
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.704
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 74.162 | 95.631 | 85.070 | 33.220 | 58.105 | 83.338 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 74.16189070851016, 'AP50': 95.6305818253082, 'AP75': 85.06993480079326, 'APs': 33.22003520453022, 'APm': 58.10488799390387, 'APl': 83.33758505125928})])}
INFO:trainer.default_trainer:This epoch takes 0:01:37.972556
INFO:trainer.default_trainer:PROGRESS: 6.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 1 training.
INFO:trainer.default_trainer:epochs[     1] optim steps[400] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01336/0.05318, loss_mask_bce_0: 0.14737/0.17277, loss_mask_dice_0: 0.47276/0.49663, loss_mask_ce_1: 0.01697/0.05366, loss_mask_bce_1: 0.15319/0.17674, loss_mask_dice_1: 0.55161/0.49999, loss_mask_ce_2: 0.01548/0.05024, loss_mask_bce_2: 0.14313/0.17504, loss_mask_dice_2: 0.70355/0.49373, loss_mask_ce_3: 0.00746/0.04857, loss_mask_bce_3: 0.14082/0.17436, loss_mask_dice_3: 0.42139/0.50292, loss_mask_ce_4: 0.01648/0.05301, loss_mask_bce_4: 0.15142/0.17520, loss_mask_dice_4: 0.51428/0.50722, loss_mask_ce_5: 0.01646/0.05967, loss_mask_bce_5: 0.15543/0.17666, loss_mask_dice_5: 0.25672/0.50779, loss_mask_ce_6: 0.01496/0.05759, loss_mask_bce_6: 0.13687/0.17990, loss_mask_dice_6: 0.35947/0.50761, loss_mask_ce_7: 0.03401/0.08360, loss_mask_bce_7: 0.16062/0.18676, loss_mask_dice_7: 0.41254/0.52098, loss_mask_ce_8: 0.03068/0.13019, loss_mask_bce_8: 0.14171/0.18596, loss_mask_dice_8: 0.43949/0.54203, loss_mask_ce_9: 0.06359/0.18566, loss_mask_bce_9: 0.17074/0.21007, loss_mask_dice_9: 0.44648/0.68651] items per batch[4] items per second[0.08] total items[1600] mini batches[   400] memory[2452] epoch remaining[0:00:57]
INFO:trainer.default_trainer:epochs[     1] optim steps[500] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00024/0.04958, loss_mask_bce_0: 0.05416/0.17764, loss_mask_dice_0: 0.18840/0.48878, loss_mask_ce_1: 0.00047/0.05085, loss_mask_bce_1: 0.05300/0.18150, loss_mask_dice_1: 0.17789/0.48857, loss_mask_ce_2: 0.00039/0.04543, loss_mask_bce_2: 0.05048/0.18018, loss_mask_dice_2: 0.16485/0.48423, loss_mask_ce_3: 0.00035/0.04696, loss_mask_bce_3: 0.04815/0.17417, loss_mask_dice_3: 0.16039/0.49295, loss_mask_ce_4: 0.00018/0.05130, loss_mask_bce_4: 0.05291/0.18109, loss_mask_dice_4: 0.18419/0.49736, loss_mask_ce_5: 0.00026/0.05618, loss_mask_bce_5: 0.06206/0.18110, loss_mask_dice_5: 0.20029/0.49489, loss_mask_ce_6: 0.00019/0.05604, loss_mask_bce_6: 0.05997/0.17981, loss_mask_dice_6: 0.22123/0.49478, loss_mask_ce_7: 0.00023/0.07875, loss_mask_bce_7: 0.05486/0.18590, loss_mask_dice_7: 0.19087/0.50955, loss_mask_ce_8: 0.00051/0.11858, loss_mask_bce_8: 0.05755/0.18710, loss_mask_dice_8: 0.20089/0.52794, loss_mask_ce_9: 0.04922/0.17377, loss_mask_bce_9: 0.06242/0.20901, loss_mask_dice_9: 0.17291/0.66928] items per batch[4] items per second[0.26] total items[2000] mini batches[   500] memory[2452] epoch remaining[0:00:39]
INFO:trainer.default_trainer:epochs[     1] optim steps[600] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00747/0.04931, loss_mask_bce_0: 0.03084/0.17354, loss_mask_dice_0: 1.10855/0.49019, loss_mask_ce_1: 0.01626/0.04974, loss_mask_bce_1: 0.03571/0.17681, loss_mask_dice_1: 1.06179/0.49167, loss_mask_ce_2: 0.02076/0.04567, loss_mask_bce_2: 0.02921/0.17562, loss_mask_dice_2: 0.86686/0.48558, loss_mask_ce_3: 0.02579/0.04737, loss_mask_bce_3: 0.03245/0.17083, loss_mask_dice_3: 0.96708/0.49032, loss_mask_ce_4: 0.03467/0.05157, loss_mask_bce_4: 0.02905/0.17689, loss_mask_dice_4: 0.88288/0.50091, loss_mask_ce_5: 0.03404/0.05677, loss_mask_bce_5: 0.03294/0.17698, loss_mask_dice_5: 0.95997/0.49536, loss_mask_ce_6: 0.18176/0.05704, loss_mask_bce_6: 0.02397/0.17584, loss_mask_dice_6: 0.63356/0.49330, loss_mask_ce_7: 0.09983/0.07844, loss_mask_bce_7: 0.03763/0.18133, loss_mask_dice_7: 0.91008/0.51181, loss_mask_ce_8: 0.03421/0.11545, loss_mask_bce_8: 0.03127/0.18382, loss_mask_dice_8: 1.16570/0.52724, loss_mask_ce_9: 0.07125/0.16617, loss_mask_bce_9: 0.02903/0.20536, loss_mask_dice_9: 1.17694/0.66178] items per batch[4] items per second[0.25] total items[2400] mini batches[   600] memory[2452] epoch remaining[0:00:23]
INFO:trainer.default_trainer:epochs[     1] optim steps[700] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00094/0.04937, loss_mask_bce_0: 0.06984/0.17112, loss_mask_dice_0: 0.35258/0.48513, loss_mask_ce_1: 0.00126/0.04866, loss_mask_bce_1: 0.07521/0.17388, loss_mask_dice_1: 0.31358/0.48867, loss_mask_ce_2: 0.00125/0.04540, loss_mask_bce_2: 0.06814/0.17278, loss_mask_dice_2: 0.29701/0.48238, loss_mask_ce_3: 0.00059/0.04541, loss_mask_bce_3: 0.07112/0.16864, loss_mask_dice_3: 0.24567/0.48630, loss_mask_ce_4: 0.00094/0.04926, loss_mask_bce_4: 0.07819/0.17410, loss_mask_dice_4: 0.27702/0.49551, loss_mask_ce_5: 0.00142/0.05678, loss_mask_bce_5: 0.07094/0.17431, loss_mask_dice_5: 0.25336/0.49132, loss_mask_ce_6: 0.00105/0.05679, loss_mask_bce_6: 0.08168/0.17329, loss_mask_dice_6: 0.33343/0.48896, loss_mask_ce_7: 0.03490/0.07930, loss_mask_bce_7: 0.07075/0.17808, loss_mask_dice_7: 0.27071/0.50984, loss_mask_ce_8: 0.00759/0.11252, loss_mask_bce_8: 0.08235/0.18142, loss_mask_dice_8: 0.29212/0.52265, loss_mask_ce_9: 0.04977/0.16229, loss_mask_bce_9: 0.08098/0.20384, loss_mask_dice_9: 0.28812/0.65257] items per batch[4] items per second[0.25] total items[2800] mini batches[   700] memory[2454] epoch remaining[0:00:07]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0009 s/iter. Inference: 0.1136 s/iter. Eval: 0.3646 s/iter. Total: 0.4792 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0013 s/iter. Inference: 0.1212 s/iter. Eval: 0.3661 s/iter. Total: 0.4886 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0014 s/iter. Inference: 0.1179 s/iter. Eval: 0.3672 s/iter. Total: 0.4866 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0014 s/iter. Inference: 0.1165 s/iter. Eval: 0.3679 s/iter. Total: 0.4859 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0014 s/iter. Inference: 0.1158 s/iter. Eval: 0.3683 s/iter. Total: 0.4856 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0014 s/iter. Inference: 0.1153 s/iter. Eval: 0.3687 s/iter. Total: 0.4855 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:36.823317
INFO:trainer.default_trainer:PROGRESS: 13.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 2 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.16s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.17 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.749
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.964
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.852
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.387
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.597
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.841
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.595
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.786
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.711
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.865
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 74.948 | 96.441 | 85.211 | 38.736 | 59.727 | 84.146 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 74.94767517514896, 'AP50': 96.44077414738031, 'AP75': 85.21115410195647, 'APs': 38.73570800960421, 'APm': 59.72718515785537, 'APl': 84.14606081733142})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.740384
INFO:trainer.default_trainer:PROGRESS: 13.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 2 training.
INFO:trainer.default_trainer:epochs[     2] optim steps[800] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00059/0.04596, loss_mask_bce_0: 0.09613/0.16742, loss_mask_dice_0: 0.24049/0.47929, loss_mask_ce_1: 0.00049/0.04652, loss_mask_bce_1: 0.08804/0.17001, loss_mask_dice_1: 0.24926/0.48283, loss_mask_ce_2: 0.00092/0.04334, loss_mask_bce_2: 0.08829/0.16920, loss_mask_dice_2: 0.27385/0.47759, loss_mask_ce_3: 0.00074/0.04277, loss_mask_bce_3: 0.09067/0.16583, loss_mask_dice_3: 0.25454/0.48132, loss_mask_ce_4: 0.00044/0.04697, loss_mask_bce_4: 0.10031/0.17042, loss_mask_dice_4: 0.27923/0.48995, loss_mask_ce_5: 0.00104/0.05354, loss_mask_bce_5: 0.11036/0.17100, loss_mask_dice_5: 0.28688/0.48615, loss_mask_ce_6: 0.00241/0.05548, loss_mask_bce_6: 0.09911/0.17054, loss_mask_dice_6: 0.28177/0.48562, loss_mask_ce_7: 0.01605/0.07776, loss_mask_bce_7: 0.09286/0.17525, loss_mask_dice_7: 0.25917/0.50417, loss_mask_ce_8: 0.01288/0.10825, loss_mask_bce_8: 0.11660/0.17767, loss_mask_dice_8: 0.26905/0.51560, loss_mask_ce_9: 0.05507/0.15544, loss_mask_bce_9: 0.12422/0.20133, loss_mask_dice_9: 0.28270/0.64134] items per batch[4] items per second[0.07] total items[3200] mini batches[   800] memory[2451] epoch remaining[0:00:52]
INFO:trainer.default_trainer:epochs[     2] optim steps[900] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00026/0.04491, loss_mask_bce_0: 0.04866/0.16920, loss_mask_dice_0: 0.25625/0.47533, loss_mask_ce_1: 0.00043/0.04523, loss_mask_bce_1: 0.04434/0.17142, loss_mask_dice_1: 0.23186/0.47897, loss_mask_ce_2: 0.00048/0.04181, loss_mask_bce_2: 0.04415/0.17155, loss_mask_dice_2: 0.18182/0.47443, loss_mask_ce_3: 0.00024/0.04217, loss_mask_bce_3: 0.04861/0.16767, loss_mask_dice_3: 0.27562/0.47775, loss_mask_ce_4: 0.00024/0.04637, loss_mask_bce_4: 0.04416/0.17251, loss_mask_dice_4: 0.20260/0.48744, loss_mask_ce_5: 0.00021/0.05364, loss_mask_bce_5: 0.04728/0.17273, loss_mask_dice_5: 0.28833/0.48221, loss_mask_ce_6: 0.00013/0.05375, loss_mask_bce_6: 0.04411/0.17241, loss_mask_dice_6: 0.21938/0.48168, loss_mask_ce_7: 0.00069/0.07548, loss_mask_bce_7: 0.04775/0.17788, loss_mask_dice_7: 0.23280/0.49892, loss_mask_ce_8: 0.00141/0.10502, loss_mask_bce_8: 0.04579/0.18132, loss_mask_dice_8: 0.27357/0.51103, loss_mask_ce_9: 0.04477/0.15050, loss_mask_bce_9: 0.04744/0.20394, loss_mask_dice_9: 0.33347/0.63239] items per batch[4] items per second[0.26] total items[3600] mini batches[   900] memory[2454] epoch remaining[0:00:35]
INFO:trainer.default_trainer:epochs[     2] optim steps[1000] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00252/0.04495, loss_mask_bce_0: 0.19725/0.16670, loss_mask_dice_0: 0.40048/0.47883, loss_mask_ce_1: 0.00387/0.04537, loss_mask_bce_1: 0.20467/0.16857, loss_mask_dice_1: 0.37355/0.48034, loss_mask_ce_2: 0.00408/0.04292, loss_mask_bce_2: 0.20924/0.16872, loss_mask_dice_2: 0.40370/0.47630, loss_mask_ce_3: 0.00512/0.04340, loss_mask_bce_3: 0.20639/0.16524, loss_mask_dice_3: 0.41592/0.47811, loss_mask_ce_4: 0.00210/0.04675, loss_mask_bce_4: 0.20288/0.16970, loss_mask_dice_4: 0.39659/0.48954, loss_mask_ce_5: 0.00137/0.05444, loss_mask_bce_5: 0.21744/0.17000, loss_mask_dice_5: 0.45692/0.48366, loss_mask_ce_6: 0.00170/0.05300, loss_mask_bce_6: 0.22740/0.16990, loss_mask_dice_6: 0.46120/0.48385, loss_mask_ce_7: 0.00197/0.07401, loss_mask_bce_7: 0.20485/0.17482, loss_mask_dice_7: 0.40102/0.49939, loss_mask_ce_8: 0.00607/0.10274, loss_mask_bce_8: 0.21328/0.17812, loss_mask_dice_8: 0.41840/0.51046, loss_mask_ce_9: 0.06221/0.14829, loss_mask_bce_9: 0.21924/0.19986, loss_mask_dice_9: 0.38914/0.62733] items per batch[4] items per second[0.26] total items[4000] mini batches[  1000] memory[2454] epoch remaining[0:00:19]
INFO:trainer.default_trainer:epochs[     2] optim steps[1100] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00065/0.04325, loss_mask_bce_0: 0.09432/0.16573, loss_mask_dice_0: 0.16371/0.47604, loss_mask_ce_1: 0.00088/0.04439, loss_mask_bce_1: 0.10701/0.16765, loss_mask_dice_1: 0.17096/0.47781, loss_mask_ce_2: 0.00094/0.04139, loss_mask_bce_2: 0.10083/0.16776, loss_mask_dice_2: 0.17244/0.47317, loss_mask_ce_3: 0.00132/0.04250, loss_mask_bce_3: 0.10139/0.16448, loss_mask_dice_3: 0.17419/0.47386, loss_mask_ce_4: 0.00158/0.04579, loss_mask_bce_4: 0.10754/0.16896, loss_mask_dice_4: 0.18360/0.48717, loss_mask_ce_5: 0.00064/0.05289, loss_mask_bce_5: 0.09657/0.16886, loss_mask_dice_5: 0.18148/0.48100, loss_mask_ce_6: 0.00065/0.05083, loss_mask_bce_6: 0.09755/0.16895, loss_mask_dice_6: 0.16417/0.48098, loss_mask_ce_7: 0.07165/0.07075, loss_mask_bce_7: 0.09588/0.17443, loss_mask_dice_7: 0.14907/0.49570, loss_mask_ce_8: 0.01852/0.09916, loss_mask_bce_8: 0.11522/0.17732, loss_mask_dice_8: 0.17995/0.50801, loss_mask_ce_9: 0.17070/0.14646, loss_mask_bce_9: 0.26961/0.19908, loss_mask_dice_9: 0.53387/0.62169] items per batch[4] items per second[0.26] total items[4400] mini batches[  1100] memory[2455] epoch remaining[0:00:03]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0008 s/iter. Inference: 0.1138 s/iter. Eval: 0.3914 s/iter. Total: 0.5060 s/iter. ETA=0:00:32
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0015 s/iter. Inference: 0.1137 s/iter. Eval: 0.3739 s/iter. Total: 0.4891 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3712 s/iter. Total: 0.4865 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0017 s/iter. Inference: 0.1135 s/iter. Eval: 0.3705 s/iter. Total: 0.4857 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0017 s/iter. Inference: 0.1134 s/iter. Eval: 0.3701 s/iter. Total: 0.4853 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0017 s/iter. Inference: 0.1133 s/iter. Eval: 0.3700 s/iter. Total: 0.4851 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:36.424428
INFO:trainer.default_trainer:PROGRESS: 20.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 3 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.16 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.959
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.856
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.837
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.595
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.785
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.697
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 74.213 | 95.878 | 85.553 | 34.518 | 58.756 | 83.726 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 74.21327933208559, 'AP50': 95.8777929768878, 'AP75': 85.55298585682921, 'APs': 34.51842955160878, 'APm': 58.75576266145568, 'APl': 83.7255528818825})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.636079
INFO:trainer.default_trainer:PROGRESS: 20.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 3 training.
INFO:trainer.default_trainer:epochs[     3] optim steps[1200] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00650/0.04105, loss_mask_bce_0: 0.06453/0.16344, loss_mask_dice_0: 0.85534/0.47372, loss_mask_ce_1: 0.00662/0.04213, loss_mask_bce_1: 0.07122/0.16546, loss_mask_dice_1: 0.99379/0.47553, loss_mask_ce_2: 0.00298/0.03971, loss_mask_bce_2: 0.06619/0.16543, loss_mask_dice_2: 0.88942/0.47081, loss_mask_ce_3: 0.00540/0.04113, loss_mask_bce_3: 0.06678/0.16261, loss_mask_dice_3: 0.89898/0.47112, loss_mask_ce_4: 0.02857/0.04463, loss_mask_bce_4: 0.06957/0.16661, loss_mask_dice_4: 0.95520/0.48264, loss_mask_ce_5: 0.01684/0.05187, loss_mask_bce_5: 0.07972/0.16667, loss_mask_dice_5: 0.87595/0.47692, loss_mask_ce_6: 0.02754/0.04977, loss_mask_bce_6: 0.08216/0.16689, loss_mask_dice_6: 0.81485/0.47837, loss_mask_ce_7: 0.22087/0.06982, loss_mask_bce_7: 0.06981/0.17183, loss_mask_dice_7: 0.88050/0.49229, loss_mask_ce_8: 0.03133/0.09540, loss_mask_bce_8: 0.07740/0.17493, loss_mask_dice_8: 0.99231/0.50451, loss_mask_ce_9: 0.07437/0.14288, loss_mask_bce_9: 0.09494/0.19635, loss_mask_dice_9: 1.83800/0.61444] items per batch[4] items per second[0.07] total items[4800] mini batches[  1200] memory[2451] epoch remaining[0:00:48]
INFO:trainer.default_trainer:epochs[     3] optim steps[1300] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00015/0.04137, loss_mask_bce_0: 0.05403/0.16431, loss_mask_dice_0: 0.34077/0.47090, loss_mask_ce_1: 0.00026/0.04165, loss_mask_bce_1: 0.06316/0.16797, loss_mask_dice_1: 0.29807/0.47297, loss_mask_ce_2: 0.00039/0.03938, loss_mask_bce_2: 0.04269/0.16802, loss_mask_dice_2: 0.29904/0.46915, loss_mask_ce_3: 0.00059/0.04073, loss_mask_bce_3: 0.04252/0.16481, loss_mask_dice_3: 0.27228/0.46782, loss_mask_ce_4: 0.00082/0.04453, loss_mask_bce_4: 0.05446/0.16909, loss_mask_dice_4: 0.29063/0.48068, loss_mask_ce_5: 0.00043/0.05266, loss_mask_bce_5: 0.06150/0.16747, loss_mask_dice_5: 0.31868/0.47299, loss_mask_ce_6: 0.00033/0.05138, loss_mask_bce_6: 0.06580/0.16755, loss_mask_dice_6: 0.31803/0.47524, loss_mask_ce_7: 0.00247/0.06974, loss_mask_bce_7: 0.04869/0.17230, loss_mask_dice_7: 0.27559/0.48888, loss_mask_ce_8: 0.07377/0.09337, loss_mask_bce_8: 0.05704/0.17653, loss_mask_dice_8: 0.31031/0.50034, loss_mask_ce_9: 0.05484/0.14093, loss_mask_bce_9: 0.05348/0.19755, loss_mask_dice_9: 0.35093/0.60886] items per batch[4] items per second[0.25] total items[5200] mini batches[  1300] memory[2453] epoch remaining[0:00:31]
INFO:trainer.default_trainer:epochs[     3] optim steps[1400] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.03274/0.04135, loss_mask_bce_0: 0.43375/0.16203, loss_mask_dice_0: 0.53511/0.47345, loss_mask_ce_1: 0.03940/0.04225, loss_mask_bce_1: 0.42362/0.16555, loss_mask_dice_1: 0.59308/0.47497, loss_mask_ce_2: 0.03510/0.03992, loss_mask_bce_2: 0.37740/0.16559, loss_mask_dice_2: 0.54474/0.47212, loss_mask_ce_3: 0.03768/0.04130, loss_mask_bce_3: 0.35993/0.16254, loss_mask_dice_3: 0.60116/0.46920, loss_mask_ce_4: 0.03274/0.04478, loss_mask_bce_4: 0.39488/0.16667, loss_mask_dice_4: 0.58846/0.48300, loss_mask_ce_5: 0.03327/0.05302, loss_mask_bce_5: 0.41206/0.16537, loss_mask_dice_5: 0.59695/0.47529, loss_mask_ce_6: 0.02720/0.05191, loss_mask_bce_6: 0.43214/0.16541, loss_mask_dice_6: 0.62187/0.47690, loss_mask_ce_7: 0.04395/0.06986, loss_mask_bce_7: 0.47904/0.17000, loss_mask_dice_7: 0.64176/0.49312, loss_mask_ce_8: 0.06264/0.09316, loss_mask_bce_8: 0.53343/0.17427, loss_mask_dice_8: 0.64240/0.50284, loss_mask_ce_9: 0.06644/0.13974, loss_mask_bce_9: 0.59135/0.19494, loss_mask_dice_9: 0.58945/0.60931] items per batch[4] items per second[0.25] total items[5600] mini batches[  1400] memory[2453] epoch remaining[0:00:15]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:epochs[     3] optim steps[1500] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.03139/0.04061, loss_mask_bce_0: 0.11162/0.16095, loss_mask_dice_0: 0.45169/0.47158, loss_mask_ce_1: 0.03866/0.04254, loss_mask_bce_1: 0.10330/0.16433, loss_mask_dice_1: 0.43496/0.47327, loss_mask_ce_2: 0.04122/0.03996, loss_mask_bce_2: 0.09879/0.16407, loss_mask_dice_2: 0.40957/0.46902, loss_mask_ce_3: 0.03076/0.04112, loss_mask_bce_3: 0.09977/0.16136, loss_mask_dice_3: 0.40745/0.46652, loss_mask_ce_4: 0.08657/0.04493, loss_mask_bce_4: 0.10123/0.16545, loss_mask_dice_4: 0.43594/0.48057, loss_mask_ce_5: 0.09939/0.05281, loss_mask_bce_5: 0.09704/0.16420, loss_mask_dice_5: 0.43068/0.47331, loss_mask_ce_6: 0.11478/0.05212, loss_mask_bce_6: 0.10939/0.16430, loss_mask_dice_6: 0.44444/0.47571, loss_mask_ce_7: 0.03626/0.06987, loss_mask_bce_7: 0.09984/0.16832, loss_mask_dice_7: 0.42247/0.49201, loss_mask_ce_8: 0.04704/0.09158, loss_mask_bce_8: 0.08459/0.17299, loss_mask_dice_8: 0.36510/0.50031, loss_mask_ce_9: 0.05934/0.13847, loss_mask_bce_9: 0.09648/0.19437, loss_mask_dice_9: 0.50909/0.60504] items per batch[4] items per second[0.25] total items[6000] mini batches[  1500] memory[2453] epoch remaining[0:00:00]
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0008 s/iter. Inference: 0.1138 s/iter. Eval: 0.3610 s/iter. Total: 0.4756 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0013 s/iter. Inference: 0.1137 s/iter. Eval: 0.3634 s/iter. Total: 0.4785 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0014 s/iter. Inference: 0.1137 s/iter. Eval: 0.3637 s/iter. Total: 0.4788 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0014 s/iter. Inference: 0.1135 s/iter. Eval: 0.3635 s/iter. Total: 0.4785 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0014 s/iter. Inference: 0.1135 s/iter. Eval: 0.3638 s/iter. Total: 0.4788 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0014 s/iter. Inference: 0.1134 s/iter. Eval: 0.3642 s/iter. Total: 0.4791 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:37.054705
INFO:trainer.default_trainer:PROGRESS: 26.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 4 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.16 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.759
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.967
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.864
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.392
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.620
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.833
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.601
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.790
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.715
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.859
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 75.868 | 96.732 | 86.417 | 39.181 | 62.027 | 83.330 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 75.86782192130104, 'AP50': 96.73166355667946, 'AP75': 86.41714743883816, 'APs': 39.18096227601329, 'APm': 62.027354228639794, 'APl': 83.32987957225468})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.929227
INFO:trainer.default_trainer:PROGRESS: 26.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 4 training.
INFO:trainer.default_trainer:epochs[     4] optim steps[1600] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00194/0.03970, loss_mask_bce_0: 0.14082/0.16202, loss_mask_dice_0: 1.14103/0.46878, loss_mask_ce_1: 0.00210/0.04121, loss_mask_bce_1: 0.16208/0.16525, loss_mask_dice_1: 0.82075/0.46940, loss_mask_ce_2: 0.00332/0.03883, loss_mask_bce_2: 0.14180/0.16497, loss_mask_dice_2: 0.73431/0.46602, loss_mask_ce_3: 0.00293/0.03991, loss_mask_bce_3: 0.15233/0.16238, loss_mask_dice_3: 1.07076/0.46397, loss_mask_ce_4: 0.00183/0.04439, loss_mask_bce_4: 0.15568/0.16645, loss_mask_dice_4: 1.12379/0.47721, loss_mask_ce_5: 0.01932/0.05315, loss_mask_bce_5: 0.16955/0.16526, loss_mask_dice_5: 1.24799/0.47025, loss_mask_ce_6: 0.02004/0.05226, loss_mask_bce_6: 0.15505/0.16560, loss_mask_dice_6: 1.05365/0.47247, loss_mask_ce_7: 0.09573/0.07080, loss_mask_bce_7: 0.15448/0.16914, loss_mask_dice_7: 0.93603/0.48883, loss_mask_ce_8: 0.01709/0.09045, loss_mask_bce_8: 0.14172/0.17401, loss_mask_dice_8: 0.59226/0.49713, loss_mask_ce_9: 0.12294/0.13707, loss_mask_bce_9: 0.16232/0.19643, loss_mask_dice_9: 0.91921/0.59938] items per batch[4] items per second[0.08] total items[6400] mini batches[  1600] memory[2452] epoch remaining[0:00:43]
INFO:trainer.default_trainer:epochs[     4] optim steps[1700] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01422/0.03917, loss_mask_bce_0: 0.01576/0.16030, loss_mask_dice_0: 0.24606/0.46559, loss_mask_ce_1: 0.01938/0.04028, loss_mask_bce_1: 0.01071/0.16334, loss_mask_dice_1: 0.19298/0.46618, loss_mask_ce_2: 0.00978/0.03858, loss_mask_bce_2: 0.01608/0.16322, loss_mask_dice_2: 0.25794/0.46381, loss_mask_ce_3: 0.00780/0.03960, loss_mask_bce_3: 0.01567/0.16069, loss_mask_dice_3: 0.29167/0.46101, loss_mask_ce_4: 0.01377/0.04373, loss_mask_bce_4: 0.01503/0.16457, loss_mask_dice_4: 0.32971/0.47363, loss_mask_ce_5: 0.01426/0.05228, loss_mask_bce_5: 0.01585/0.16353, loss_mask_dice_5: 0.28841/0.46840, loss_mask_ce_6: 0.01019/0.05344, loss_mask_bce_6: 0.02403/0.16391, loss_mask_dice_6: 0.31329/0.46895, loss_mask_ce_7: 0.02282/0.06998, loss_mask_bce_7: 0.01755/0.16753, loss_mask_dice_7: 0.33947/0.48680, loss_mask_ce_8: 0.00959/0.08948, loss_mask_bce_8: 0.01386/0.17205, loss_mask_dice_8: 0.27377/0.49514, loss_mask_ce_9: 0.05420/0.13585, loss_mask_bce_9: 0.01132/0.19420, loss_mask_dice_9: 0.27039/0.59258] items per batch[4] items per second[0.26] total items[6800] mini batches[  1700] memory[2455] epoch remaining[0:00:27]
INFO:trainer.default_trainer:epochs[     4] optim steps[1800] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00924/0.03969, loss_mask_bce_0: 0.23806/0.15974, loss_mask_dice_0: 0.42641/0.46728, loss_mask_ce_1: 0.01146/0.04065, loss_mask_bce_1: 0.26457/0.16290, loss_mask_dice_1: 0.45974/0.46852, loss_mask_ce_2: 0.01569/0.03888, loss_mask_bce_2: 0.26098/0.16286, loss_mask_dice_2: 0.44181/0.46588, loss_mask_ce_3: 0.02090/0.03974, loss_mask_bce_3: 0.26113/0.16022, loss_mask_dice_3: 0.43325/0.46353, loss_mask_ce_4: 0.01138/0.04423, loss_mask_bce_4: 0.23913/0.16413, loss_mask_dice_4: 0.40487/0.47591, loss_mask_ce_5: 0.00345/0.05362, loss_mask_bce_5: 0.19044/0.16316, loss_mask_dice_5: 0.29998/0.47089, loss_mask_ce_6: 0.01753/0.05446, loss_mask_bce_6: 0.21123/0.16371, loss_mask_dice_6: 0.34094/0.47199, loss_mask_ce_7: 0.01326/0.07130, loss_mask_bce_7: 0.18704/0.16731, loss_mask_dice_7: 0.32705/0.49012, loss_mask_ce_8: 0.01725/0.08796, loss_mask_bce_8: 0.31990/0.17176, loss_mask_dice_8: 0.51685/0.49759, loss_mask_ce_9: 0.17928/0.13520, loss_mask_bce_9: 0.21248/0.19361, loss_mask_dice_9: 0.30358/0.59283] items per batch[4] items per second[0.26] total items[7200] mini batches[  1800] memory[2456] epoch remaining[0:00:11]
WARNING:trainer.utils_trainer:Saving checkpoint...
WARNING:trainer.utils_trainer:Saving checkpoint...
WARNING:trainer.utils_trainer:Finished saving checkpoint and model to output/focall_unicl_lang_v1.yaml_conf~/run_6/00001875.
INFO:trainer.default_trainer:Evaluation start ...
WARNING:trainer.utils_trainer:Finished saving checkpoint and model to output/focall_unicl_lang_v1.yaml_conf~/run_6/00001875.
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0011 s/iter. Inference: 0.1140 s/iter. Eval: 0.3722 s/iter. Total: 0.4873 s/iter. ETA=0:00:31
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0015 s/iter. Inference: 0.1139 s/iter. Eval: 0.3710 s/iter. Total: 0.4864 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1139 s/iter. Eval: 0.3709 s/iter. Total: 0.4865 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1137 s/iter. Eval: 0.3712 s/iter. Total: 0.4866 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3712 s/iter. Total: 0.4865 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 65/75. Dataloading: 0.0017 s/iter. Inference: 0.1135 s/iter. Eval: 0.3740 s/iter. Total: 0.4892 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:37.028623
INFO:trainer.default_trainer:PROGRESS: 33.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 5 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.07 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.13s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.14 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.752
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.867
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.621
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.840
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.601
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.792
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.495
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.860
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 75.244 | 96.884 | 86.705 | 37.076 | 62.117 | 84.016 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 75.24402340500616, 'AP50': 96.88429562950228, 'AP75': 86.70492458996807, 'APs': 37.07590684500528, 'APm': 62.11659265375013, 'APl': 84.01589241108954})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.937213
INFO:trainer.default_trainer:PROGRESS: 33.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 5 training.
INFO:trainer.default_trainer:epochs[     5] optim steps[1900] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.07737/0.03914, loss_mask_bce_0: 0.09768/0.15862, loss_mask_dice_0: 0.29533/0.46392, loss_mask_ce_1: 0.08162/0.04022, loss_mask_bce_1: 0.10927/0.16177, loss_mask_dice_1: 0.35835/0.46533, loss_mask_ce_2: 0.06436/0.03828, loss_mask_bce_2: 0.11227/0.16176, loss_mask_dice_2: 0.42598/0.46299, loss_mask_ce_3: 0.03253/0.03863, loss_mask_bce_3: 0.10713/0.15919, loss_mask_dice_3: 0.56574/0.46076, loss_mask_ce_4: 0.03515/0.04359, loss_mask_bce_4: 0.10291/0.16310, loss_mask_dice_4: 0.54802/0.47261, loss_mask_ce_5: 0.03657/0.05328, loss_mask_bce_5: 0.09989/0.16195, loss_mask_dice_5: 0.28626/0.46834, loss_mask_ce_6: 0.06036/0.05488, loss_mask_bce_6: 0.10716/0.16253, loss_mask_dice_6: 0.55236/0.47046, loss_mask_ce_7: 0.03782/0.07177, loss_mask_bce_7: 0.10844/0.16607, loss_mask_dice_7: 0.44862/0.48650, loss_mask_ce_8: 0.04267/0.08669, loss_mask_bce_8: 0.11484/0.17042, loss_mask_dice_8: 0.36095/0.49408, loss_mask_ce_9: 0.05488/0.13381, loss_mask_bce_9: 0.14428/0.19286, loss_mask_dice_9: 0.39582/0.58834] items per batch[4] items per second[0.07] total items[7600] mini batches[  1900] memory[2450] epoch remaining[0:00:57]
INFO:trainer.default_trainer:epochs[     5] optim steps[2000] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00022/0.03854, loss_mask_bce_0: 0.04689/0.15887, loss_mask_dice_0: 0.17487/0.46318, loss_mask_ce_1: 0.00031/0.03936, loss_mask_bce_1: 0.04424/0.16188, loss_mask_dice_1: 0.13593/0.46365, loss_mask_ce_2: 0.00034/0.03770, loss_mask_bce_2: 0.05378/0.16174, loss_mask_dice_2: 0.18094/0.46178, loss_mask_ce_3: 0.00039/0.03826, loss_mask_bce_3: 0.05144/0.15931, loss_mask_dice_3: 0.16809/0.45906, loss_mask_ce_4: 0.00050/0.04306, loss_mask_bce_4: 0.04530/0.16359, loss_mask_dice_4: 0.15931/0.47116, loss_mask_ce_5: 0.00155/0.05299, loss_mask_bce_5: 0.05415/0.16216, loss_mask_dice_5: 0.17661/0.46683, loss_mask_ce_6: 0.00148/0.05544, loss_mask_bce_6: 0.04378/0.16256, loss_mask_dice_6: 0.17673/0.46857, loss_mask_ce_7: 0.00079/0.07127, loss_mask_bce_7: 0.04458/0.16617, loss_mask_dice_7: 0.13470/0.48355, loss_mask_ce_8: 0.00111/0.08477, loss_mask_bce_8: 0.05094/0.17050, loss_mask_dice_8: 0.17581/0.49142, loss_mask_ce_9: 0.04347/0.13276, loss_mask_bce_9: 0.04466/0.19283, loss_mask_dice_9: 0.17861/0.58483] items per batch[4] items per second[0.25] total items[8000] mini batches[  2000] memory[2452] epoch remaining[0:00:39]
INFO:trainer.default_trainer:epochs[     5] optim steps[2100] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.06311/0.03862, loss_mask_bce_0: 0.03219/0.15821, loss_mask_dice_0: 0.89610/0.46408, loss_mask_ce_1: 0.04568/0.03912, loss_mask_bce_1: 0.03502/0.16122, loss_mask_dice_1: 1.11577/0.46482, loss_mask_ce_2: 0.05697/0.03762, loss_mask_bce_2: 0.02909/0.16121, loss_mask_dice_2: 0.85056/0.46273, loss_mask_ce_3: 0.05288/0.03878, loss_mask_bce_3: 0.03167/0.15883, loss_mask_dice_3: 1.04194/0.46081, loss_mask_ce_4: 0.08215/0.04316, loss_mask_bce_4: 0.02965/0.16276, loss_mask_dice_4: 0.74365/0.47125, loss_mask_ce_5: 0.05431/0.05381, loss_mask_bce_5: 0.02991/0.16144, loss_mask_dice_5: 0.72767/0.46742, loss_mask_ce_6: 0.04015/0.05552, loss_mask_bce_6: 0.03202/0.16217, loss_mask_dice_6: 0.95672/0.46946, loss_mask_ce_7: 0.03923/0.07091, loss_mask_bce_7: 0.04755/0.16566, loss_mask_dice_7: 1.20961/0.48339, loss_mask_ce_8: 0.13094/0.08352, loss_mask_bce_8: 0.03865/0.16993, loss_mask_dice_8: 0.83472/0.49186, loss_mask_ce_9: 0.11418/0.13185, loss_mask_bce_9: 0.02503/0.19186, loss_mask_dice_9: 0.65787/0.58392] items per batch[4] items per second[0.25] total items[8400] mini batches[  2100] memory[2452] epoch remaining[0:00:23]
INFO:trainer.default_trainer:epochs[     5] optim steps[2200] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00119/0.03868, loss_mask_bce_0: 0.06100/0.15759, loss_mask_dice_0: 0.24994/0.46330, loss_mask_ce_1: 0.00162/0.03923, loss_mask_bce_1: 0.06921/0.16046, loss_mask_dice_1: 0.22714/0.46423, loss_mask_ce_2: 0.00178/0.03766, loss_mask_bce_2: 0.06769/0.16049, loss_mask_dice_2: 0.26282/0.46223, loss_mask_ce_3: 0.00143/0.03848, loss_mask_bce_3: 0.07162/0.15824, loss_mask_dice_3: 0.26757/0.46020, loss_mask_ce_4: 0.00086/0.04309, loss_mask_bce_4: 0.07299/0.16213, loss_mask_dice_4: 0.23870/0.47084, loss_mask_ce_5: 0.02671/0.05398, loss_mask_bce_5: 0.06900/0.16096, loss_mask_dice_5: 0.31245/0.46645, loss_mask_ce_6: 0.00092/0.05603, loss_mask_bce_6: 0.06564/0.16171, loss_mask_dice_6: 0.25191/0.46866, loss_mask_ce_7: 0.00759/0.07033, loss_mask_bce_7: 0.06391/0.16481, loss_mask_dice_7: 0.21627/0.48220, loss_mask_ce_8: 0.01126/0.08270, loss_mask_bce_8: 0.07923/0.16924, loss_mask_dice_8: 0.27677/0.49093, loss_mask_ce_9: 0.05523/0.13191, loss_mask_bce_9: 0.08343/0.19171, loss_mask_dice_9: 0.33407/0.58249] items per batch[4] items per second[0.25] total items[8800] mini batches[  2200] memory[2452] epoch remaining[0:00:07]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0010 s/iter. Inference: 0.1141 s/iter. Eval: 0.3629 s/iter. Total: 0.4781 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0015 s/iter. Inference: 0.1138 s/iter. Eval: 0.3644 s/iter. Total: 0.4798 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1137 s/iter. Eval: 0.3651 s/iter. Total: 0.4805 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1135 s/iter. Eval: 0.3657 s/iter. Total: 0.4809 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0017 s/iter. Inference: 0.1134 s/iter. Eval: 0.3656 s/iter. Total: 0.4808 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0017 s/iter. Inference: 0.1134 s/iter. Eval: 0.3659 s/iter. Total: 0.4810 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:36.886420
INFO:trainer.default_trainer:PROGRESS: 40.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 6 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.13s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.14 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.754
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.968
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.864
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.627
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.840
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.600
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.804
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.475
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.707
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.858
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 75.443 | 96.807 | 86.422 | 36.396 | 62.728 | 84.040 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 75.44266578886854, 'AP50': 96.8065671778344, 'AP75': 86.4217585051135, 'APs': 36.395771441874004, 'APm': 62.728379491312666, 'APl': 84.03966257459973})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.911378
INFO:trainer.default_trainer:PROGRESS: 40.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 6 training.
INFO:trainer.default_trainer:epochs[     6] optim steps[2300] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00013/0.03771, loss_mask_bce_0: 0.10541/0.15659, loss_mask_dice_0: 0.22838/0.46122, loss_mask_ce_1: 0.00030/0.03827, loss_mask_bce_1: 0.11015/0.15948, loss_mask_dice_1: 0.27134/0.46219, loss_mask_ce_2: 0.00063/0.03687, loss_mask_bce_2: 0.10411/0.15941, loss_mask_dice_2: 0.27051/0.46048, loss_mask_ce_3: 0.00071/0.03767, loss_mask_bce_3: 0.10719/0.15724, loss_mask_dice_3: 0.27137/0.45856, loss_mask_ce_4: 0.00033/0.04237, loss_mask_bce_4: 0.11421/0.16105, loss_mask_dice_4: 0.25899/0.46858, loss_mask_ce_5: 0.03494/0.05373, loss_mask_bce_5: 0.11131/0.15996, loss_mask_dice_5: 0.31650/0.46500, loss_mask_ce_6: 0.00032/0.05599, loss_mask_bce_6: 0.11516/0.16076, loss_mask_dice_6: 0.28008/0.46675, loss_mask_ce_7: 0.00027/0.06913, loss_mask_bce_7: 0.10948/0.16361, loss_mask_dice_7: 0.28561/0.48013, loss_mask_ce_8: 0.00263/0.08093, loss_mask_bce_8: 0.12085/0.16827, loss_mask_dice_8: 0.26360/0.48894, loss_mask_ce_9: 0.05040/0.13018, loss_mask_bce_9: 0.11158/0.19093, loss_mask_dice_9: 0.28989/0.57853] items per batch[4] items per second[0.07] total items[9200] mini batches[  2300] memory[2450] epoch remaining[0:00:52]
INFO:trainer.default_trainer:epochs[     6] optim steps[2400] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00052/0.03744, loss_mask_bce_0: 0.05011/0.15706, loss_mask_dice_0: 0.22160/0.46084, loss_mask_ce_1: 0.00031/0.03791, loss_mask_bce_1: 0.04299/0.15996, loss_mask_dice_1: 0.19696/0.46214, loss_mask_ce_2: 0.00033/0.03690, loss_mask_bce_2: 0.05010/0.15977, loss_mask_dice_2: 0.22554/0.46042, loss_mask_ce_3: 0.00044/0.03731, loss_mask_bce_3: 0.04305/0.15759, loss_mask_dice_3: 0.15392/0.45757, loss_mask_ce_4: 0.00078/0.04223, loss_mask_bce_4: 0.04685/0.16133, loss_mask_dice_4: 0.23722/0.46747, loss_mask_ce_5: 0.00081/0.05378, loss_mask_bce_5: 0.05095/0.16031, loss_mask_dice_5: 0.23581/0.46477, loss_mask_ce_6: 0.00162/0.05645, loss_mask_bce_6: 0.04352/0.16126, loss_mask_dice_6: 0.25664/0.46679, loss_mask_ce_7: 0.00202/0.06890, loss_mask_bce_7: 0.05092/0.16397, loss_mask_dice_7: 0.33526/0.47901, loss_mask_ce_8: 0.00714/0.08069, loss_mask_bce_8: 0.05391/0.16877, loss_mask_dice_8: 0.26181/0.48728, loss_mask_ce_9: 0.03607/0.12907, loss_mask_bce_9: 0.04642/0.19118, loss_mask_dice_9: 0.31356/0.57612] items per batch[4] items per second[0.25] total items[9600] mini batches[  2400] memory[2453] epoch remaining[0:00:35]
INFO:trainer.default_trainer:epochs[     6] optim steps[2500] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00180/0.03775, loss_mask_bce_0: 0.24083/0.15628, loss_mask_dice_0: 0.40126/0.46219, loss_mask_ce_1: 0.00220/0.03821, loss_mask_bce_1: 0.22768/0.15906, loss_mask_dice_1: 0.44017/0.46193, loss_mask_ce_2: 0.00233/0.03710, loss_mask_bce_2: 0.22511/0.15891, loss_mask_dice_2: 0.45560/0.46038, loss_mask_ce_3: 0.00574/0.03765, loss_mask_bce_3: 0.21212/0.15675, loss_mask_dice_3: 0.42902/0.45730, loss_mask_ce_4: 0.06769/0.04209, loss_mask_bce_4: 0.23268/0.16039, loss_mask_dice_4: 0.41562/0.46850, loss_mask_ce_5: 0.03453/0.05422, loss_mask_bce_5: 0.24554/0.15962, loss_mask_dice_5: 0.44556/0.46576, loss_mask_ce_6: 0.01059/0.05674, loss_mask_bce_6: 0.19833/0.16030, loss_mask_dice_6: 0.43830/0.46754, loss_mask_ce_7: 0.02582/0.06924, loss_mask_bce_7: 0.22867/0.16319, loss_mask_dice_7: 0.38549/0.47891, loss_mask_ce_8: 0.02078/0.08046, loss_mask_bce_8: 0.22962/0.16769, loss_mask_dice_8: 0.40532/0.48786, loss_mask_ce_9: 0.09191/0.12890, loss_mask_bce_9: 0.26341/0.19016, loss_mask_dice_9: 0.41349/0.57546] items per batch[4] items per second[0.26] total items[10000] mini batches[  2500] memory[2453] epoch remaining[0:00:19]
INFO:trainer.default_trainer:epochs[     6] optim steps[2600] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00023/0.03763, loss_mask_bce_0: 0.11097/0.15588, loss_mask_dice_0: 0.16507/0.46061, loss_mask_ce_1: 0.00045/0.03829, loss_mask_bce_1: 0.10582/0.15854, loss_mask_dice_1: 0.16050/0.46027, loss_mask_ce_2: 0.00040/0.03750, loss_mask_bce_2: 0.10014/0.15829, loss_mask_dice_2: 0.14821/0.45830, loss_mask_ce_3: 0.00083/0.03773, loss_mask_bce_3: 0.11123/0.15624, loss_mask_dice_3: 0.16384/0.45545, loss_mask_ce_4: 0.03189/0.04219, loss_mask_bce_4: 0.10060/0.15969, loss_mask_dice_4: 0.16074/0.46683, loss_mask_ce_5: 0.03360/0.05498, loss_mask_bce_5: 0.10504/0.15921, loss_mask_dice_5: 0.16518/0.46461, loss_mask_ce_6: 0.01907/0.05761, loss_mask_bce_6: 0.10697/0.15985, loss_mask_dice_6: 0.17317/0.46614, loss_mask_ce_7: 0.00471/0.06977, loss_mask_bce_7: 0.10474/0.16265, loss_mask_dice_7: 0.16018/0.47709, loss_mask_ce_8: 0.00267/0.08010, loss_mask_bce_8: 0.10947/0.16722, loss_mask_dice_8: 0.18004/0.48678, loss_mask_ce_9: 0.09771/0.12894, loss_mask_bce_9: 0.13245/0.19008, loss_mask_dice_9: 0.19860/0.57421] items per batch[4] items per second[0.25] total items[10400] mini batches[  2600] memory[2453] epoch remaining[0:00:03]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0008 s/iter. Inference: 0.1139 s/iter. Eval: 0.3716 s/iter. Total: 0.4863 s/iter. ETA=0:00:31
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0014 s/iter. Inference: 0.1138 s/iter. Eval: 0.3711 s/iter. Total: 0.4863 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0015 s/iter. Inference: 0.1138 s/iter. Eval: 0.3737 s/iter. Total: 0.4892 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3744 s/iter. Total: 0.4897 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1135 s/iter. Eval: 0.3754 s/iter. Total: 0.4906 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0016 s/iter. Inference: 0.1134 s/iter. Eval: 0.3761 s/iter. Total: 0.4913 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:37.535813
INFO:trainer.default_trainer:PROGRESS: 46.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 7 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.13s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.13 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.754
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.965
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.862
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.359
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.594
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.843
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.598
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.794
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.706
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.866
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 75.414 | 96.482 | 86.176 | 35.891 | 59.424 | 84.347 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 75.41431266011534, 'AP50': 96.48180931342493, 'AP75': 86.17569437846203, 'APs': 35.891160453789944, 'APm': 59.42438509088526, 'APl': 84.34729420120335})])}
INFO:trainer.default_trainer:This epoch takes 0:01:37.650642
INFO:trainer.default_trainer:PROGRESS: 46.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 7 training.
INFO:trainer.default_trainer:epochs[     7] optim steps[2700] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01628/0.03709, loss_mask_bce_0: 0.05178/0.15535, loss_mask_dice_0: 0.68356/0.46036, loss_mask_ce_1: 0.02571/0.03764, loss_mask_bce_1: 0.05930/0.15792, loss_mask_dice_1: 0.78892/0.45902, loss_mask_ce_2: 0.03290/0.03689, loss_mask_bce_2: 0.05092/0.15768, loss_mask_dice_2: 0.59515/0.45725, loss_mask_ce_3: 0.04566/0.03715, loss_mask_bce_3: 0.04889/0.15572, loss_mask_dice_3: 0.63194/0.45440, loss_mask_ce_4: 0.13660/0.04254, loss_mask_bce_4: 0.04592/0.15908, loss_mask_dice_4: 0.75246/0.46668, loss_mask_ce_5: 0.04458/0.05502, loss_mask_bce_5: 0.04115/0.15878, loss_mask_dice_5: 0.70325/0.46460, loss_mask_ce_6: 0.10451/0.05781, loss_mask_bce_6: 0.03717/0.15949, loss_mask_dice_6: 0.61736/0.46459, loss_mask_ce_7: 0.09795/0.06924, loss_mask_bce_7: 0.05268/0.16208, loss_mask_dice_7: 0.73388/0.47630, loss_mask_ce_8: 0.04838/0.07867, loss_mask_bce_8: 0.05346/0.16645, loss_mask_dice_8: 0.75237/0.48609, loss_mask_ce_9: 0.08650/0.12784, loss_mask_bce_9: 0.07380/0.18923, loss_mask_dice_9: 1.07949/0.57138] items per batch[4] items per second[0.07] total items[10800] mini batches[  2700] memory[2451] epoch remaining[0:00:47]
INFO:trainer.default_trainer:epochs[     7] optim steps[2800] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00037/0.03727, loss_mask_bce_0: 0.03555/0.15562, loss_mask_dice_0: 0.26006/0.45886, loss_mask_ce_1: 0.00068/0.03786, loss_mask_bce_1: 0.05462/0.15806, loss_mask_dice_1: 0.31665/0.45722, loss_mask_ce_2: 0.00077/0.03699, loss_mask_bce_2: 0.06050/0.15799, loss_mask_dice_2: 0.34320/0.45625, loss_mask_ce_3: 0.00123/0.03730, loss_mask_bce_3: 0.04575/0.15616, loss_mask_dice_3: 0.28802/0.45349, loss_mask_ce_4: 0.00690/0.04329, loss_mask_bce_4: 0.05626/0.15957, loss_mask_dice_4: 0.28713/0.46570, loss_mask_ce_5: 0.03434/0.05590, loss_mask_bce_5: 0.05034/0.15926, loss_mask_dice_5: 0.30406/0.46324, loss_mask_ce_6: 0.00106/0.05809, loss_mask_bce_6: 0.04585/0.16009, loss_mask_dice_6: 0.24935/0.46248, loss_mask_ce_7: 0.00436/0.06925, loss_mask_bce_7: 0.06442/0.16236, loss_mask_dice_7: 0.38298/0.47447, loss_mask_ce_8: 0.00098/0.07790, loss_mask_bce_8: 0.06434/0.16680, loss_mask_dice_8: 0.32961/0.48465, loss_mask_ce_9: 0.04892/0.12788, loss_mask_bce_9: 0.05003/0.18914, loss_mask_dice_9: 0.33720/0.56859] items per batch[4] items per second[0.25] total items[11200] mini batches[  2800] memory[2455] epoch remaining[0:00:31]
INFO:trainer.default_trainer:epochs[     7] optim steps[2900] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.02815/0.03707, loss_mask_bce_0: 0.12291/0.15473, loss_mask_dice_0: 0.34039/0.46025, loss_mask_ce_1: 0.04557/0.03805, loss_mask_bce_1: 0.14402/0.15709, loss_mask_dice_1: 0.42168/0.45811, loss_mask_ce_2: 0.03076/0.03707, loss_mask_bce_2: 0.12351/0.15707, loss_mask_dice_2: 0.31607/0.45768, loss_mask_ce_3: 0.03461/0.03733, loss_mask_bce_3: 0.12197/0.15532, loss_mask_dice_3: 0.38360/0.45485, loss_mask_ce_4: 0.04948/0.04380, loss_mask_bce_4: 0.11158/0.15862, loss_mask_dice_4: 0.36402/0.46650, loss_mask_ce_5: 0.09126/0.05660, loss_mask_bce_5: 0.11123/0.15860, loss_mask_dice_5: 0.30223/0.46457, loss_mask_ce_6: 0.08882/0.05784, loss_mask_bce_6: 0.12276/0.15928, loss_mask_dice_6: 0.30911/0.46328, loss_mask_ce_7: 0.08481/0.06894, loss_mask_bce_7: 0.13072/0.16148, loss_mask_dice_7: 0.38575/0.47630, loss_mask_ce_8: 0.05328/0.07739, loss_mask_bce_8: 0.12059/0.16595, loss_mask_dice_8: 0.33121/0.48557, loss_mask_ce_9: 0.06093/0.12775, loss_mask_bce_9: 0.14539/0.18801, loss_mask_dice_9: 0.45100/0.57031] items per batch[4] items per second[0.25] total items[11600] mini batches[  2900] memory[2455] epoch remaining[0:00:15]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:epochs[     7] optim steps[3000] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.02466/0.03694, loss_mask_bce_0: 0.11401/0.15455, loss_mask_dice_0: 0.50795/0.45913, loss_mask_ce_1: 0.03740/0.03781, loss_mask_bce_1: 0.10673/0.15682, loss_mask_dice_1: 0.47005/0.45723, loss_mask_ce_2: 0.03998/0.03701, loss_mask_bce_2: 0.10854/0.15674, loss_mask_dice_2: 0.49263/0.45685, loss_mask_ce_3: 0.03544/0.03746, loss_mask_bce_3: 0.09367/0.15502, loss_mask_dice_3: 0.45374/0.45328, loss_mask_ce_4: 0.02925/0.04432, loss_mask_bce_4: 0.08559/0.15829, loss_mask_dice_4: 0.46810/0.46612, loss_mask_ce_5: 0.04780/0.05685, loss_mask_bce_5: 0.10590/0.15842, loss_mask_dice_5: 0.59596/0.46389, loss_mask_ce_6: 0.04711/0.05757, loss_mask_bce_6: 0.08656/0.15900, loss_mask_dice_6: 0.43322/0.46281, loss_mask_ce_7: 0.06738/0.06858, loss_mask_bce_7: 0.09144/0.16115, loss_mask_dice_7: 0.42231/0.47556, loss_mask_ce_8: 0.02778/0.07691, loss_mask_bce_8: 0.08500/0.16564, loss_mask_dice_8: 0.43619/0.48481, loss_mask_ce_9: 0.04524/0.12732, loss_mask_bce_9: 0.11179/0.18771, loss_mask_dice_9: 0.39063/0.56799] items per batch[4] items per second[0.25] total items[12000] mini batches[  3000] memory[2455] epoch remaining[0:00:00]
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0013 s/iter. Inference: 0.1138 s/iter. Eval: 0.3572 s/iter. Total: 0.4723 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0015 s/iter. Inference: 0.1136 s/iter. Eval: 0.3578 s/iter. Total: 0.4730 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3587 s/iter. Total: 0.4739 s/iter. ETA=0:00:19
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1134 s/iter. Eval: 0.3592 s/iter. Total: 0.4742 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1133 s/iter. Eval: 0.3586 s/iter. Total: 0.4737 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0016 s/iter. Inference: 0.1133 s/iter. Eval: 0.3601 s/iter. Total: 0.4750 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:36.385251
INFO:trainer.default_trainer:PROGRESS: 53.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 8 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.14s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.15 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.749
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.953
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.866
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.328
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.619
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.841
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.597
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.794
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.812
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.864
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 74.872 | 95.318 | 86.599 | 32.764 | 61.906 | 84.085 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 74.87216409277968, 'AP50': 95.31754187454425, 'AP75': 86.59896340678486, 'APs': 32.763612956396315, 'APm': 61.905704225802424, 'APl': 84.08524982126717})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.542536
INFO:trainer.default_trainer:PROGRESS: 53.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 8 training.
INFO:trainer.default_trainer:epochs[     8] optim steps[3100] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.10914/0.03679, loss_mask_bce_0: 0.12521/0.15478, loss_mask_dice_0: 0.49062/0.45760, loss_mask_ce_1: 0.12851/0.03811, loss_mask_bce_1: 0.13047/0.15688, loss_mask_dice_1: 0.65907/0.45566, loss_mask_ce_2: 0.12392/0.03701, loss_mask_bce_2: 0.13590/0.15687, loss_mask_dice_2: 0.76336/0.45571, loss_mask_ce_3: 0.01603/0.03741, loss_mask_bce_3: 0.17663/0.15506, loss_mask_dice_3: 1.31223/0.45241, loss_mask_ce_4: 0.00731/0.04474, loss_mask_bce_4: 0.14582/0.15819, loss_mask_dice_4: 0.90097/0.46497, loss_mask_ce_5: 0.02156/0.05721, loss_mask_bce_5: 0.15889/0.15862, loss_mask_dice_5: 0.99895/0.46262, loss_mask_ce_6: 0.15902/0.05738, loss_mask_bce_6: 0.13589/0.15900, loss_mask_dice_6: 0.82263/0.46103, loss_mask_ce_7: 0.00399/0.06809, loss_mask_bce_7: 0.13372/0.16128, loss_mask_dice_7: 0.99542/0.47436, loss_mask_ce_8: 0.02577/0.07597, loss_mask_bce_8: 0.14856/0.16571, loss_mask_dice_8: 1.25730/0.48383, loss_mask_ce_9: 0.11725/0.12632, loss_mask_bce_9: 0.12617/0.18874, loss_mask_dice_9: 0.88341/0.56662] items per batch[4] items per second[0.08] total items[12400] mini batches[  3100] memory[2452] epoch remaining[0:00:43]
INFO:trainer.default_trainer:epochs[     8] optim steps[3200] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00312/0.03692, loss_mask_bce_0: 0.02372/0.15403, loss_mask_dice_0: 0.34063/0.45649, loss_mask_ce_1: 0.00270/0.03822, loss_mask_bce_1: 0.01492/0.15609, loss_mask_dice_1: 0.26653/0.45428, loss_mask_ce_2: 0.00169/0.03689, loss_mask_bce_2: 0.02175/0.15609, loss_mask_dice_2: 0.30382/0.45466, loss_mask_ce_3: 0.00155/0.03761, loss_mask_bce_3: 0.02153/0.15431, loss_mask_dice_3: 0.29275/0.45122, loss_mask_ce_4: 0.00538/0.04494, loss_mask_bce_4: 0.02615/0.15743, loss_mask_dice_4: 0.45951/0.46424, loss_mask_ce_5: 0.00792/0.05724, loss_mask_bce_5: 0.03043/0.15793, loss_mask_dice_5: 0.34580/0.46154, loss_mask_ce_6: 0.00731/0.05724, loss_mask_bce_6: 0.02102/0.15820, loss_mask_dice_6: 0.29190/0.45944, loss_mask_ce_7: 0.04946/0.06763, loss_mask_bce_7: 0.01959/0.16046, loss_mask_dice_7: 0.25768/0.47340, loss_mask_ce_8: 0.00764/0.07545, loss_mask_bce_8: 0.02142/0.16488, loss_mask_dice_8: 0.22444/0.48254, loss_mask_ce_9: 0.04365/0.12581, loss_mask_bce_9: 0.01201/0.18764, loss_mask_dice_9: 0.25283/0.56449] items per batch[4] items per second[0.25] total items[12800] mini batches[  3200] memory[2452] epoch remaining[0:00:27]
INFO:trainer.default_trainer:epochs[     8] optim steps[3300] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00038/0.03735, loss_mask_bce_0: 0.11860/0.15356, loss_mask_dice_0: 0.24074/0.45749, loss_mask_ce_1: 0.00011/0.03852, loss_mask_bce_1: 0.13607/0.15554, loss_mask_dice_1: 0.25968/0.45527, loss_mask_ce_2: 0.00014/0.03745, loss_mask_bce_2: 0.11899/0.15558, loss_mask_dice_2: 0.23656/0.45573, loss_mask_ce_3: 0.00050/0.03751, loss_mask_bce_3: 0.13028/0.15380, loss_mask_dice_3: 0.24947/0.45244, loss_mask_ce_4: 0.00094/0.04517, loss_mask_bce_4: 0.12616/0.15699, loss_mask_dice_4: 0.24767/0.46475, loss_mask_ce_5: 0.00193/0.05736, loss_mask_bce_5: 0.12371/0.15762, loss_mask_dice_5: 0.25949/0.46206, loss_mask_ce_6: 0.00059/0.05754, loss_mask_bce_6: 0.12959/0.15786, loss_mask_dice_6: 0.23803/0.46043, loss_mask_ce_7: 0.00206/0.06782, loss_mask_bce_7: 0.11494/0.15989, loss_mask_dice_7: 0.23213/0.47415, loss_mask_ce_8: 0.00232/0.07520, loss_mask_bce_8: 0.12205/0.16431, loss_mask_dice_8: 0.24715/0.48305, loss_mask_ce_9: 0.06705/0.12572, loss_mask_bce_9: 0.14738/0.18694, loss_mask_dice_9: 0.24684/0.56467] items per batch[4] items per second[0.25] total items[13200] mini batches[  3300] memory[2453] epoch remaining[0:00:11]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0008 s/iter. Inference: 0.1140 s/iter. Eval: 0.3735 s/iter. Total: 0.4883 s/iter. ETA=0:00:31
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0013 s/iter. Inference: 0.1139 s/iter. Eval: 0.3764 s/iter. Total: 0.4917 s/iter. ETA=0:00:26
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0014 s/iter. Inference: 0.1139 s/iter. Eval: 0.3753 s/iter. Total: 0.4906 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0014 s/iter. Inference: 0.1138 s/iter. Eval: 0.3751 s/iter. Total: 0.4904 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0014 s/iter. Inference: 0.1137 s/iter. Eval: 0.3754 s/iter. Total: 0.4906 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0014 s/iter. Inference: 0.1136 s/iter. Eval: 0.3760 s/iter. Total: 0.4911 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:37.884919
INFO:trainer.default_trainer:PROGRESS: 60.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 9 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.16s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.17 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.967
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.877
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.358
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.627
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.846
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.604
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.719
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.872
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 76.180 | 96.724 | 87.658 | 35.782 | 62.729 | 84.561 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 76.17989221874205, 'AP50': 96.72369948345764, 'AP75': 87.65803976281912, 'APs': 35.782374238328046, 'APm': 62.72864601451327, 'APl': 84.56139162615601})])}
INFO:trainer.default_trainer:This epoch takes 0:01:37.887229
INFO:trainer.default_trainer:PROGRESS: 60.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 9 training.
INFO:trainer.default_trainer:epochs[     9] optim steps[3400] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01280/0.03686, loss_mask_bce_0: 0.11492/0.15320, loss_mask_dice_0: 0.38474/0.45668, loss_mask_ce_1: 0.01459/0.03813, loss_mask_bce_1: 0.11296/0.15520, loss_mask_dice_1: 0.25039/0.45426, loss_mask_ce_2: 0.02138/0.03712, loss_mask_bce_2: 0.12116/0.15515, loss_mask_dice_2: 0.27088/0.45415, loss_mask_ce_3: 0.02077/0.03724, loss_mask_bce_3: 0.11995/0.15342, loss_mask_dice_3: 0.31597/0.45121, loss_mask_ce_4: 0.02269/0.04514, loss_mask_bce_4: 0.10725/0.15663, loss_mask_dice_4: 0.20205/0.46344, loss_mask_ce_5: 0.01423/0.05706, loss_mask_bce_5: 0.10877/0.15723, loss_mask_dice_5: 0.55932/0.46098, loss_mask_ce_6: 0.01970/0.05675, loss_mask_bce_6: 0.12283/0.15749, loss_mask_dice_6: 0.38027/0.45911, loss_mask_ce_7: 0.01133/0.06687, loss_mask_bce_7: 0.11346/0.15955, loss_mask_dice_7: 0.59228/0.47233, loss_mask_ce_8: 0.00396/0.07439, loss_mask_bce_8: 0.11032/0.16384, loss_mask_dice_8: 0.27995/0.48132, loss_mask_ce_9: 0.05246/0.12505, loss_mask_bce_9: 0.13978/0.18657, loss_mask_dice_9: 0.50770/0.56208] items per batch[4] items per second[0.07] total items[13600] mini batches[  3400] memory[2450] epoch remaining[0:00:57]
INFO:trainer.default_trainer:epochs[     9] optim steps[3500] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00044/0.03667, loss_mask_bce_0: 0.05919/0.15320, loss_mask_dice_0: 0.19892/0.45628, loss_mask_ce_1: 0.00035/0.03804, loss_mask_bce_1: 0.04696/0.15514, loss_mask_dice_1: 0.16613/0.45392, loss_mask_ce_2: 0.00051/0.03700, loss_mask_bce_2: 0.05437/0.15508, loss_mask_dice_2: 0.17854/0.45322, loss_mask_ce_3: 0.00051/0.03724, loss_mask_bce_3: 0.05369/0.15345, loss_mask_dice_3: 0.17064/0.45053, loss_mask_ce_4: 0.00102/0.04564, loss_mask_bce_4: 0.05559/0.15705, loss_mask_dice_4: 0.18235/0.46246, loss_mask_ce_5: 0.00040/0.05679, loss_mask_bce_5: 0.05515/0.15716, loss_mask_dice_5: 0.14362/0.46068, loss_mask_ce_6: 0.00009/0.05662, loss_mask_bce_6: 0.05895/0.15735, loss_mask_dice_6: 0.16161/0.45798, loss_mask_ce_7: 0.00582/0.06658, loss_mask_bce_7: 0.04791/0.15951, loss_mask_dice_7: 0.18171/0.47188, loss_mask_ce_8: 0.00146/0.07418, loss_mask_bce_8: 0.05667/0.16398, loss_mask_dice_8: 0.18632/0.48074, loss_mask_ce_9: 0.05548/0.12480, loss_mask_bce_9: 0.06561/0.18645, loss_mask_dice_9: 0.19716/0.56132] items per batch[4] items per second[0.25] total items[14000] mini batches[  3500] memory[2450] epoch remaining[0:00:40]
INFO:trainer.default_trainer:epochs[     9] optim steps[3600] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00235/0.03684, loss_mask_bce_0: 0.04923/0.15284, loss_mask_dice_0: 0.98486/0.45678, loss_mask_ce_1: 0.00368/0.03798, loss_mask_bce_1: 0.04576/0.15476, loss_mask_dice_1: 1.44393/0.45463, loss_mask_ce_2: 0.00400/0.03758, loss_mask_bce_2: 0.03877/0.15472, loss_mask_dice_2: 1.11907/0.45307, loss_mask_ce_3: 0.00525/0.03718, loss_mask_bce_3: 0.04467/0.15308, loss_mask_dice_3: 0.95986/0.45101, loss_mask_ce_4: 0.00826/0.04672, loss_mask_bce_4: 0.05558/0.15657, loss_mask_dice_4: 1.00669/0.46273, loss_mask_ce_5: 0.00717/0.05694, loss_mask_bce_5: 0.05653/0.15687, loss_mask_dice_5: 1.12919/0.46168, loss_mask_ce_6: 0.00382/0.05644, loss_mask_bce_6: 0.04607/0.15688, loss_mask_dice_6: 1.13778/0.45895, loss_mask_ce_7: 0.01568/0.06661, loss_mask_bce_7: 0.05699/0.15909, loss_mask_dice_7: 1.10892/0.47259, loss_mask_ce_8: 0.01175/0.07401, loss_mask_bce_8: 0.05061/0.16351, loss_mask_dice_8: 1.29476/0.48158, loss_mask_ce_9: 0.04675/0.12486, loss_mask_bce_9: 0.04660/0.18617, loss_mask_dice_9: 1.19016/0.56200] items per batch[4] items per second[0.26] total items[14400] mini batches[  3600] memory[2451] epoch remaining[0:00:23]
INFO:trainer.default_trainer:epochs[     9] optim steps[3700] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00037/0.03644, loss_mask_bce_0: 0.06340/0.15238, loss_mask_dice_0: 0.24713/0.45627, loss_mask_ce_1: 0.00050/0.03759, loss_mask_bce_1: 0.05955/0.15431, loss_mask_dice_1: 0.25218/0.45426, loss_mask_ce_2: 0.00086/0.03735, loss_mask_bce_2: 0.06155/0.15422, loss_mask_dice_2: 0.25855/0.45258, loss_mask_ce_3: 0.00132/0.03697, loss_mask_bce_3: 0.06202/0.15268, loss_mask_dice_3: 0.24209/0.45053, loss_mask_ce_4: 0.00196/0.04675, loss_mask_bce_4: 0.06350/0.15615, loss_mask_dice_4: 0.27464/0.46222, loss_mask_ce_5: 0.00872/0.05712, loss_mask_bce_5: 0.06403/0.15650, loss_mask_dice_5: 0.24843/0.46132, loss_mask_ce_6: 0.00118/0.05589, loss_mask_bce_6: 0.06645/0.15651, loss_mask_dice_6: 0.20411/0.45909, loss_mask_ce_7: 0.00153/0.06654, loss_mask_bce_7: 0.06463/0.15874, loss_mask_dice_7: 0.24439/0.47255, loss_mask_ce_8: 0.00076/0.07351, loss_mask_bce_8: 0.06051/0.16320, loss_mask_dice_8: 0.20866/0.48166, loss_mask_ce_9: 0.05670/0.12506, loss_mask_bce_9: 0.07707/0.18570, loss_mask_dice_9: 0.20166/0.56165] items per batch[4] items per second[0.25] total items[14800] mini batches[  3700] memory[2451] epoch remaining[0:00:07]
WARNING:trainer.utils_trainer:Saving checkpoint...
WARNING:trainer.utils_trainer:Saving checkpoint...
WARNING:trainer.utils_trainer:Finished saving checkpoint and model to output/focall_unicl_lang_v1.yaml_conf~/run_6/00003750.
INFO:trainer.default_trainer:Evaluation start ...
WARNING:trainer.utils_trainer:Finished saving checkpoint and model to output/focall_unicl_lang_v1.yaml_conf~/run_6/00003750.
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0010 s/iter. Inference: 0.1139 s/iter. Eval: 0.3715 s/iter. Total: 0.4864 s/iter. ETA=0:00:31
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0015 s/iter. Inference: 0.1139 s/iter. Eval: 0.3756 s/iter. Total: 0.4911 s/iter. ETA=0:00:26
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1138 s/iter. Eval: 0.3760 s/iter. Total: 0.4915 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1137 s/iter. Eval: 0.3759 s/iter. Total: 0.4913 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3765 s/iter. Total: 0.4918 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0017 s/iter. Inference: 0.1135 s/iter. Eval: 0.3769 s/iter. Total: 0.4922 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:38.198494
INFO:trainer.default_trainer:PROGRESS: 66.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 10 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.14s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.15 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.759
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.872
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.609
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.845
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.598
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.812
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.475
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.703
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 75.852 | 96.913 | 87.187 | 34.127 | 60.902 | 84.517 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 75.85152169611685, 'AP50': 96.91254293690736, 'AP75': 87.18680751208392, 'APs': 34.12711564639882, 'APm': 60.902046034507464, 'APl': 84.51692681794144})])}
INFO:trainer.default_trainer:This epoch takes 0:01:37.919569
INFO:trainer.default_trainer:PROGRESS: 66.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 10 training.
INFO:trainer.default_trainer:epochs[    10] optim steps[3800] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00060/0.03622, loss_mask_bce_0: 0.09626/0.15200, loss_mask_dice_0: 0.27179/0.45618, loss_mask_ce_1: 0.00059/0.03719, loss_mask_bce_1: 0.09040/0.15395, loss_mask_dice_1: 0.26706/0.45405, loss_mask_ce_2: 0.00059/0.03704, loss_mask_bce_2: 0.09786/0.15383, loss_mask_dice_2: 0.33972/0.45230, loss_mask_ce_3: 0.00116/0.03679, loss_mask_bce_3: 0.08595/0.15227, loss_mask_dice_3: 0.28345/0.45000, loss_mask_ce_4: 0.02013/0.04695, loss_mask_bce_4: 0.09023/0.15573, loss_mask_dice_4: 0.26224/0.46129, loss_mask_ce_5: 0.00534/0.05663, loss_mask_bce_5: 0.08885/0.15613, loss_mask_dice_5: 0.27771/0.46093, loss_mask_ce_6: 0.00144/0.05562, loss_mask_bce_6: 0.08741/0.15609, loss_mask_dice_6: 0.29901/0.45853, loss_mask_ce_7: 0.00159/0.06594, loss_mask_bce_7: 0.09518/0.15852, loss_mask_dice_7: 0.25024/0.47193, loss_mask_ce_8: 0.00413/0.07293, loss_mask_bce_8: 0.10986/0.16284, loss_mask_dice_8: 0.35276/0.48083, loss_mask_ce_9: 0.05717/0.12495, loss_mask_bce_9: 0.10689/0.18532, loss_mask_dice_9: 0.24642/0.55987] items per batch[4] items per second[0.07] total items[15200] mini batches[  3800] memory[2449] epoch remaining[0:00:52]
INFO:trainer.default_trainer:epochs[    10] optim steps[3900] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00054/0.03622, loss_mask_bce_0: 0.04208/0.15227, loss_mask_dice_0: 0.24710/0.45612, loss_mask_ce_1: 0.00055/0.03715, loss_mask_bce_1: 0.04653/0.15460, loss_mask_dice_1: 0.25271/0.45408, loss_mask_ce_2: 0.00094/0.03689, loss_mask_bce_2: 0.04782/0.15440, loss_mask_dice_2: 0.20531/0.45199, loss_mask_ce_3: 0.00100/0.03670, loss_mask_bce_3: 0.04247/0.15290, loss_mask_dice_3: 0.17695/0.44962, loss_mask_ce_4: 0.00113/0.04715, loss_mask_bce_4: 0.04729/0.15574, loss_mask_dice_4: 0.23595/0.46079, loss_mask_ce_5: 0.00226/0.05637, loss_mask_bce_5: 0.05026/0.15625, loss_mask_dice_5: 0.22088/0.46006, loss_mask_ce_6: 0.00137/0.05516, loss_mask_bce_6: 0.04639/0.15622, loss_mask_dice_6: 0.24523/0.45810, loss_mask_ce_7: 0.00404/0.06556, loss_mask_bce_7: 0.04122/0.15875, loss_mask_dice_7: 0.20400/0.47134, loss_mask_ce_8: 0.00206/0.07278, loss_mask_bce_8: 0.05132/0.16300, loss_mask_dice_8: 0.37839/0.48009, loss_mask_ce_9: 0.04755/0.12448, loss_mask_bce_9: 0.05051/0.18544, loss_mask_dice_9: 0.23590/0.55811] items per batch[4] items per second[0.25] total items[15600] mini batches[  3900] memory[2451] epoch remaining[0:00:36]
INFO:trainer.default_trainer:epochs[    10] optim steps[4000] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00043/0.03643, loss_mask_bce_0: 0.18776/0.15179, loss_mask_dice_0: 0.34530/0.45621, loss_mask_ce_1: 0.00030/0.03721, loss_mask_bce_1: 0.19435/0.15408, loss_mask_dice_1: 0.36626/0.45479, loss_mask_ce_2: 0.00019/0.03692, loss_mask_bce_2: 0.18776/0.15393, loss_mask_dice_2: 0.34441/0.45213, loss_mask_ce_3: 0.00027/0.03691, loss_mask_bce_3: 0.18497/0.15244, loss_mask_dice_3: 0.34137/0.45000, loss_mask_ce_4: 0.00063/0.04732, loss_mask_bce_4: 0.20959/0.15525, loss_mask_dice_4: 0.35559/0.46093, loss_mask_ce_5: 0.00132/0.05655, loss_mask_bce_5: 0.20879/0.15576, loss_mask_dice_5: 0.33968/0.46024, loss_mask_ce_6: 0.00044/0.05518, loss_mask_bce_6: 0.19149/0.15570, loss_mask_dice_6: 0.36803/0.45829, loss_mask_ce_7: 0.00042/0.06523, loss_mask_bce_7: 0.19872/0.15816, loss_mask_dice_7: 0.31191/0.47138, loss_mask_ce_8: 0.00271/0.07231, loss_mask_bce_8: 0.20756/0.16244, loss_mask_dice_8: 0.33058/0.48005, loss_mask_ce_9: 0.07173/0.12443, loss_mask_bce_9: 0.24051/0.18468, loss_mask_dice_9: 0.41771/0.55789] items per batch[4] items per second[0.25] total items[16000] mini batches[  4000] memory[2452] epoch remaining[0:00:20]
INFO:trainer.default_trainer:epochs[    10] optim steps[4100] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00030/0.03598, loss_mask_bce_0: 0.10742/0.15148, loss_mask_dice_0: 0.18265/0.45528, loss_mask_ce_1: 0.00025/0.03699, loss_mask_bce_1: 0.11304/0.15374, loss_mask_dice_1: 0.18076/0.45402, loss_mask_ce_2: 0.00021/0.03684, loss_mask_bce_2: 0.10308/0.15360, loss_mask_dice_2: 0.16799/0.45110, loss_mask_ce_3: 0.00031/0.03666, loss_mask_bce_3: 0.10661/0.15211, loss_mask_dice_3: 0.17534/0.44929, loss_mask_ce_4: 0.00092/0.04688, loss_mask_bce_4: 0.10966/0.15489, loss_mask_dice_4: 0.17569/0.45996, loss_mask_ce_5: 0.00096/0.05625, loss_mask_bce_5: 0.10881/0.15546, loss_mask_dice_5: 0.18723/0.45947, loss_mask_ce_6: 0.00079/0.05501, loss_mask_bce_6: 0.11443/0.15533, loss_mask_dice_6: 0.18792/0.45750, loss_mask_ce_7: 0.00133/0.06488, loss_mask_bce_7: 0.11554/0.15778, loss_mask_dice_7: 0.18968/0.47074, loss_mask_ce_8: 0.00149/0.07183, loss_mask_bce_8: 0.09978/0.16227, loss_mask_dice_8: 0.17532/0.47939, loss_mask_ce_9: 0.08083/0.12456, loss_mask_bce_9: 0.11359/0.18433, loss_mask_dice_9: 0.17532/0.55674] items per batch[4] items per second[0.25] total items[16400] mini batches[  4100] memory[2452] epoch remaining[0:00:04]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0010 s/iter. Inference: 0.1142 s/iter. Eval: 0.3679 s/iter. Total: 0.4831 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0014 s/iter. Inference: 0.1139 s/iter. Eval: 0.3691 s/iter. Total: 0.4845 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1138 s/iter. Eval: 0.3705 s/iter. Total: 0.4860 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1137 s/iter. Eval: 0.3717 s/iter. Total: 0.4871 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3752 s/iter. Total: 0.4905 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0017 s/iter. Inference: 0.1135 s/iter. Eval: 0.3749 s/iter. Total: 0.4901 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:38.670807
INFO:trainer.default_trainer:PROGRESS: 73.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 11 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.16 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.765
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.972
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.877
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.384
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.637
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.609
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.802
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.814
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.704
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 76.475 | 97.162 | 87.747 | 38.356 | 63.733 | 85.427 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 76.47545032515644, 'AP50': 97.16192589837225, 'AP75': 87.74681415573072, 'APs': 38.35615182959348, 'APm': 63.732722203390516, 'APl': 85.42737282803095})])}
INFO:trainer.default_trainer:This epoch takes 0:01:38.644663
INFO:trainer.default_trainer:PROGRESS: 73.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 11 training.
INFO:trainer.default_trainer:epochs[    11] optim steps[4200] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00248/0.03568, loss_mask_bce_0: 0.10513/0.15102, loss_mask_dice_0: 0.89551/0.45461, loss_mask_ce_1: 0.00484/0.03663, loss_mask_bce_1: 0.08338/0.15322, loss_mask_dice_1: 0.83948/0.45300, loss_mask_ce_2: 0.00702/0.03636, loss_mask_bce_2: 0.10293/0.15309, loss_mask_dice_2: 0.87861/0.45020, loss_mask_ce_3: 0.00353/0.03627, loss_mask_bce_3: 0.08964/0.15164, loss_mask_dice_3: 0.91646/0.44845, loss_mask_ce_4: 0.00286/0.04670, loss_mask_bce_4: 0.09067/0.15438, loss_mask_dice_4: 1.01711/0.45901, loss_mask_ce_5: 0.01028/0.05545, loss_mask_bce_5: 0.08769/0.15493, loss_mask_dice_5: 0.89148/0.45859, loss_mask_ce_6: 0.00422/0.05429, loss_mask_bce_6: 0.09218/0.15485, loss_mask_dice_6: 0.86454/0.45654, loss_mask_ce_7: 0.00490/0.06431, loss_mask_bce_7: 0.06494/0.15728, loss_mask_dice_7: 0.89072/0.46973, loss_mask_ce_8: 0.02336/0.07142, loss_mask_bce_8: 0.06684/0.16183, loss_mask_dice_8: 0.75825/0.47907, loss_mask_ce_9: 0.07610/0.12359, loss_mask_bce_9: 0.13378/0.18374, loss_mask_dice_9: 1.44838/0.55551] items per batch[4] items per second[0.07] total items[16800] mini batches[  4200] memory[2453] epoch remaining[0:00:47]
INFO:trainer.default_trainer:epochs[    11] optim steps[4300] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00046/0.03529, loss_mask_bce_0: 0.05505/0.15146, loss_mask_dice_0: 0.32473/0.45362, loss_mask_ce_1: 0.00132/0.03639, loss_mask_bce_1: 0.06863/0.15372, loss_mask_dice_1: 0.42547/0.45238, loss_mask_ce_2: 0.00291/0.03632, loss_mask_bce_2: 0.07229/0.15348, loss_mask_dice_2: 0.40613/0.44929, loss_mask_ce_3: 0.00090/0.03598, loss_mask_bce_3: 0.06218/0.15210, loss_mask_dice_3: 0.34834/0.44783, loss_mask_ce_4: 0.00406/0.04639, loss_mask_bce_4: 0.05955/0.15480, loss_mask_dice_4: 0.32300/0.45865, loss_mask_ce_5: 0.00160/0.05510, loss_mask_bce_5: 0.06477/0.15526, loss_mask_dice_5: 0.33133/0.45782, loss_mask_ce_6: 0.00087/0.05395, loss_mask_bce_6: 0.05610/0.15525, loss_mask_dice_6: 0.37377/0.45568, loss_mask_ce_7: 0.00305/0.06423, loss_mask_bce_7: 0.07991/0.15764, loss_mask_dice_7: 0.37438/0.46923, loss_mask_ce_8: 0.00254/0.07095, loss_mask_bce_8: 0.07169/0.16232, loss_mask_dice_8: 0.38258/0.47841, loss_mask_ce_9: 0.05066/0.12358, loss_mask_bce_9: 0.05947/0.18387, loss_mask_dice_9: 0.39980/0.55385] items per batch[4] items per second[0.26] total items[17200] mini batches[  4300] memory[2453] epoch remaining[0:00:31]
INFO:trainer.default_trainer:epochs[    11] optim steps[4400] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01764/0.03540, loss_mask_bce_0: 0.16053/0.15119, loss_mask_dice_0: 0.42978/0.45441, loss_mask_ce_1: 0.01472/0.03661, loss_mask_bce_1: 0.15798/0.15335, loss_mask_dice_1: 0.43643/0.45380, loss_mask_ce_2: 0.01780/0.03647, loss_mask_bce_2: 0.17008/0.15313, loss_mask_dice_2: 0.41208/0.44988, loss_mask_ce_3: 0.01394/0.03602, loss_mask_bce_3: 0.13508/0.15148, loss_mask_dice_3: 0.34818/0.44857, loss_mask_ce_4: 0.02066/0.04636, loss_mask_bce_4: 0.15199/0.15428, loss_mask_dice_4: 0.43263/0.45934, loss_mask_ce_5: 0.04332/0.05488, loss_mask_bce_5: 0.17454/0.15477, loss_mask_dice_5: 0.44910/0.45814, loss_mask_ce_6: 0.03863/0.05402, loss_mask_bce_6: 0.17393/0.15476, loss_mask_dice_6: 0.46028/0.45667, loss_mask_ce_7: 0.26938/0.06388, loss_mask_bce_7: 0.17907/0.15726, loss_mask_dice_7: 0.41998/0.47027, loss_mask_ce_8: 0.06075/0.07076, loss_mask_bce_8: 0.21246/0.16187, loss_mask_dice_8: 0.64448/0.47918, loss_mask_ce_9: 0.05472/0.12334, loss_mask_bce_9: 0.18571/0.18321, loss_mask_dice_9: 0.47786/0.55461] items per batch[4] items per second[0.25] total items[17600] mini batches[  4400] memory[2453] epoch remaining[0:00:15]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:epochs[    11] optim steps[4500] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.02832/0.03514, loss_mask_bce_0: 0.10644/0.15080, loss_mask_dice_0: 0.51952/0.45334, loss_mask_ce_1: 0.03543/0.03637, loss_mask_bce_1: 0.09598/0.15289, loss_mask_dice_1: 0.48165/0.45270, loss_mask_ce_2: 0.03275/0.03627, loss_mask_bce_2: 0.10323/0.15269, loss_mask_dice_2: 0.43693/0.44902, loss_mask_ce_3: 0.03835/0.03587, loss_mask_bce_3: 0.10063/0.15111, loss_mask_dice_3: 0.44286/0.44796, loss_mask_ce_4: 0.03741/0.04612, loss_mask_bce_4: 0.09864/0.15386, loss_mask_dice_4: 0.43635/0.45847, loss_mask_ce_5: 0.04850/0.05461, loss_mask_bce_5: 0.09625/0.15434, loss_mask_dice_5: 0.41917/0.45719, loss_mask_ce_6: 0.03556/0.05363, loss_mask_bce_6: 0.10587/0.15429, loss_mask_dice_6: 0.45215/0.45586, loss_mask_ce_7: 0.02749/0.06366, loss_mask_bce_7: 0.08942/0.15678, loss_mask_dice_7: 0.51110/0.46908, loss_mask_ce_8: 0.03348/0.07008, loss_mask_bce_8: 0.12261/0.16144, loss_mask_dice_8: 0.42892/0.47842, loss_mask_ce_9: 0.05164/0.12356, loss_mask_bce_9: 0.10324/0.18274, loss_mask_dice_9: 0.47187/0.55262] items per batch[4] items per second[0.26] total items[18000] mini batches[  4500] memory[2453] epoch remaining[0:00:00]
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0012 s/iter. Inference: 0.1141 s/iter. Eval: 0.3691 s/iter. Total: 0.4844 s/iter. ETA=0:00:31
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0015 s/iter. Inference: 0.1139 s/iter. Eval: 0.3707 s/iter. Total: 0.4862 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1139 s/iter. Eval: 0.3713 s/iter. Total: 0.4868 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1138 s/iter. Eval: 0.3717 s/iter. Total: 0.4872 s/iter. ETA=0:00:15
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1137 s/iter. Eval: 0.3717 s/iter. Total: 0.4871 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 65/75. Dataloading: 0.0017 s/iter. Inference: 0.1135 s/iter. Eval: 0.3744 s/iter. Total: 0.4897 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:36.856200
INFO:trainer.default_trainer:PROGRESS: 80.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 12 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.08 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.16s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.17 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.764
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.874
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.619
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.851
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.609
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.799
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.817
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.510
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.715
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 76.385 | 97.729 | 87.380 | 36.550 | 61.928 | 85.095 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 76.38515033582117, 'AP50': 97.7292129089216, 'AP75': 87.38039209936174, 'APs': 36.55027160653516, 'APm': 61.92794386366734, 'APl': 85.09526137574647})])}
INFO:trainer.default_trainer:This epoch takes 0:01:36.986543
INFO:trainer.default_trainer:PROGRESS: 80.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 12 training.
INFO:trainer.default_trainer:epochs[    12] optim steps[4600] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00696/0.03508, loss_mask_bce_0: 0.13517/0.15088, loss_mask_dice_0: 1.08558/0.45234, loss_mask_ce_1: 0.69814/0.03629, loss_mask_bce_1: 0.13024/0.15329, loss_mask_dice_1: 0.43043/0.45162, loss_mask_ce_2: 0.44653/0.03614, loss_mask_bce_2: 0.12751/0.15265, loss_mask_dice_2: 0.47238/0.44788, loss_mask_ce_3: 0.00442/0.03567, loss_mask_bce_3: 0.15838/0.15138, loss_mask_dice_3: 1.08456/0.44689, loss_mask_ce_4: 0.02934/0.04586, loss_mask_bce_4: 0.14178/0.15382, loss_mask_dice_4: 0.91472/0.45743, loss_mask_ce_5: 0.02276/0.05429, loss_mask_bce_5: 0.14797/0.15424, loss_mask_dice_5: 1.09902/0.45608, loss_mask_ce_6: 0.00708/0.05347, loss_mask_bce_6: 0.13672/0.15422, loss_mask_dice_6: 0.81746/0.45470, loss_mask_ce_7: 0.02645/0.06337, loss_mask_bce_7: 0.12662/0.15677, loss_mask_dice_7: 0.73639/0.46779, loss_mask_ce_8: 0.01257/0.06988, loss_mask_bce_8: 0.12584/0.16128, loss_mask_dice_8: 0.74081/0.47684, loss_mask_ce_9: 0.12743/0.12341, loss_mask_bce_9: 0.13200/0.18270, loss_mask_dice_9: 0.86411/0.55120] items per batch[4] items per second[0.07] total items[18400] mini batches[  4600] memory[2451] epoch remaining[0:00:44]
INFO:trainer.default_trainer:epochs[    12] optim steps[4700] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00153/0.03474, loss_mask_bce_0: 0.02782/0.15037, loss_mask_dice_0: 0.40080/0.45127, loss_mask_ce_1: 0.00338/0.03591, loss_mask_bce_1: 0.02840/0.15279, loss_mask_dice_1: 0.29764/0.45090, loss_mask_ce_2: 0.00114/0.03579, loss_mask_bce_2: 0.02405/0.15215, loss_mask_dice_2: 0.29268/0.44693, loss_mask_ce_3: 0.00174/0.03540, loss_mask_bce_3: 0.02629/0.15086, loss_mask_dice_3: 0.36130/0.44590, loss_mask_ce_4: 0.00136/0.04589, loss_mask_bce_4: 0.02425/0.15330, loss_mask_dice_4: 0.32375/0.45633, loss_mask_ce_5: 0.00137/0.05392, loss_mask_bce_5: 0.02928/0.15370, loss_mask_dice_5: 0.46942/0.45525, loss_mask_ce_6: 0.00079/0.05313, loss_mask_bce_6: 0.02040/0.15367, loss_mask_dice_6: 0.31459/0.45350, loss_mask_ce_7: 0.00095/0.06262, loss_mask_bce_7: 0.02554/0.15621, loss_mask_dice_7: 0.33336/0.46643, loss_mask_ce_8: 0.00338/0.06913, loss_mask_bce_8: 0.01888/0.16071, loss_mask_dice_8: 0.29510/0.47575, loss_mask_ce_9: 0.04760/0.12306, loss_mask_bce_9: 0.01702/0.18198, loss_mask_dice_9: 0.36846/0.54916] items per batch[4] items per second[0.25] total items[18800] mini batches[  4700] memory[2452] epoch remaining[0:00:27]
INFO:trainer.default_trainer:epochs[    12] optim steps[4800] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.00115/0.03475, loss_mask_bce_0: 0.13250/0.15003, loss_mask_dice_0: 0.25139/0.45168, loss_mask_ce_1: 0.00694/0.03614, loss_mask_bce_1: 0.14046/0.15242, loss_mask_dice_1: 0.25715/0.45103, loss_mask_ce_2: 0.01938/0.03588, loss_mask_bce_2: 0.12545/0.15180, loss_mask_dice_2: 0.23986/0.44723, loss_mask_ce_3: 0.02505/0.03570, loss_mask_bce_3: 0.13076/0.15053, loss_mask_dice_3: 0.24214/0.44599, loss_mask_ce_4: 0.01018/0.04601, loss_mask_bce_4: 0.12628/0.15292, loss_mask_dice_4: 0.24313/0.45643, loss_mask_ce_5: 0.00234/0.05411, loss_mask_bce_5: 0.12231/0.15336, loss_mask_dice_5: 0.24037/0.45546, loss_mask_ce_6: 0.00086/0.05326, loss_mask_bce_6: 0.13802/0.15328, loss_mask_dice_6: 0.25578/0.45387, loss_mask_ce_7: 0.00058/0.06269, loss_mask_bce_7: 0.13069/0.15583, loss_mask_dice_7: 0.26065/0.46692, loss_mask_ce_8: 0.02315/0.06935, loss_mask_bce_8: 0.13356/0.16035, loss_mask_dice_8: 0.24642/0.47576, loss_mask_ce_9: 0.06980/0.12303, loss_mask_bce_9: 0.16489/0.18147, loss_mask_dice_9: 0.28517/0.54929] items per batch[4] items per second[0.25] total items[19200] mini batches[  4800] memory[2453] epoch remaining[0:00:11]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0007 s/iter. Inference: 0.1139 s/iter. Eval: 0.3631 s/iter. Total: 0.4777 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0013 s/iter. Inference: 0.1138 s/iter. Eval: 0.3642 s/iter. Total: 0.4794 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0014 s/iter. Inference: 0.1138 s/iter. Eval: 0.3642 s/iter. Total: 0.4794 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0014 s/iter. Inference: 0.1137 s/iter. Eval: 0.3640 s/iter. Total: 0.4793 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0014 s/iter. Inference: 0.1136 s/iter. Eval: 0.3640 s/iter. Total: 0.4791 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0014 s/iter. Inference: 0.1135 s/iter. Eval: 0.3644 s/iter. Total: 0.4795 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:37.086305
INFO:trainer.default_trainer:PROGRESS: 86.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 13 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.07 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.16 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.759
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.875
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.620
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.847
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.604
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.796
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.817
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.485
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.721
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.872
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 75.890 | 96.920 | 87.505 | 37.980 | 62.044 | 84.708 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 75.89015560419539, 'AP50': 96.92004764270654, 'AP75': 87.5046316035372, 'APs': 37.97969135152605, 'APm': 62.04415966819576, 'APl': 84.70822586727974})])}
INFO:trainer.default_trainer:This epoch takes 0:01:37.056038
INFO:trainer.default_trainer:PROGRESS: 86.67%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 13 training.
INFO:trainer.default_trainer:epochs[    13] optim steps[4900] learning rate[default: 1.00000e-03] train loss[loss_mask_ce_0: 0.01263/0.03430, loss_mask_bce_0: 0.11557/0.14967, loss_mask_dice_0: 0.30591/0.45054, loss_mask_ce_1: 0.01021/0.03567, loss_mask_bce_1: 0.12185/0.15208, loss_mask_dice_1: 0.29959/0.44972, loss_mask_ce_2: 0.01533/0.03540, loss_mask_bce_2: 0.11274/0.15145, loss_mask_dice_2: 0.71062/0.44646, loss_mask_ce_3: 0.00795/0.03524, loss_mask_bce_3: 0.12327/0.15021, loss_mask_dice_3: 0.41645/0.44530, loss_mask_ce_4: 0.03807/0.04544, loss_mask_bce_4: 0.12525/0.15256, loss_mask_dice_4: 0.64596/0.45546, loss_mask_ce_5: 0.01704/0.05334, loss_mask_bce_5: 0.12886/0.15305, loss_mask_dice_5: 0.56263/0.45470, loss_mask_ce_6: 0.01666/0.05248, loss_mask_bce_6: 0.12501/0.15300, loss_mask_dice_6: 0.40983/0.45313, loss_mask_ce_7: 0.01602/0.06190, loss_mask_bce_7: 0.13737/0.15547, loss_mask_dice_7: 0.32495/0.46544, loss_mask_ce_8: 0.01253/0.06872, loss_mask_bce_8: 0.13128/0.15997, loss_mask_dice_8: 0.42639/0.47487, loss_mask_ce_9: 0.06047/0.12262, loss_mask_bce_9: 0.14144/0.18127, loss_mask_dice_9: 0.53543/0.54799] items per batch[4] items per second[0.07] total items[19600] mini batches[  4900] memory[2449] epoch remaining[0:00:58]
INFO:trainer.default_trainer:epochs[    13] optim steps[5000] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.00003/0.03417, loss_mask_bce_0: 0.04706/0.14970, loss_mask_dice_0: 0.17343/0.44961, loss_mask_ce_1: 0.00007/0.03551, loss_mask_bce_1: 0.04467/0.15211, loss_mask_dice_1: 0.15237/0.44889, loss_mask_ce_2: 0.00006/0.03523, loss_mask_bce_2: 0.04679/0.15136, loss_mask_dice_2: 0.18934/0.44573, loss_mask_ce_3: 0.00023/0.03535, loss_mask_bce_3: 0.05150/0.15031, loss_mask_dice_3: 0.18595/0.44446, loss_mask_ce_4: 0.01720/0.04522, loss_mask_bce_4: 0.04651/0.15244, loss_mask_dice_4: 0.14748/0.45471, loss_mask_ce_5: 0.00015/0.05292, loss_mask_bce_5: 0.05598/0.15294, loss_mask_dice_5: 0.16937/0.45365, loss_mask_ce_6: 0.00006/0.05211, loss_mask_bce_6: 0.04563/0.15281, loss_mask_dice_6: 0.16391/0.45217, loss_mask_ce_7: 0.00022/0.06135, loss_mask_bce_7: 0.04743/0.15542, loss_mask_dice_7: 0.16866/0.46446, loss_mask_ce_8: 0.00042/0.06858, loss_mask_bce_8: 0.04373/0.15987, loss_mask_dice_8: 0.16182/0.47396, loss_mask_ce_9: 0.04851/0.12208, loss_mask_bce_9: 0.05800/0.18110, loss_mask_dice_9: 0.18663/0.54678] items per batch[4] items per second[0.25] total items[20000] mini batches[  5000] memory[2451] epoch remaining[0:00:39]
INFO:trainer.default_trainer:epochs[    13] optim steps[5100] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.00305/0.03427, loss_mask_bce_0: 0.02981/0.14947, loss_mask_dice_0: 0.82669/0.44958, loss_mask_ce_1: 0.00164/0.03546, loss_mask_bce_1: 0.03019/0.15190, loss_mask_dice_1: 0.65881/0.44878, loss_mask_ce_2: 0.00275/0.03529, loss_mask_bce_2: 0.01882/0.15113, loss_mask_dice_2: 0.69154/0.44550, loss_mask_ce_3: 0.00216/0.03531, loss_mask_bce_3: 0.02944/0.15009, loss_mask_dice_3: 0.68373/0.44442, loss_mask_ce_4: 0.00417/0.04503, loss_mask_bce_4: 0.02804/0.15224, loss_mask_dice_4: 0.91332/0.45476, loss_mask_ce_5: 0.00501/0.05267, loss_mask_bce_5: 0.03183/0.15279, loss_mask_dice_5: 0.87878/0.45381, loss_mask_ce_6: 0.00353/0.05220, loss_mask_bce_6: 0.02173/0.15259, loss_mask_dice_6: 0.72471/0.45198, loss_mask_ce_7: 0.00845/0.06112, loss_mask_bce_7: 0.02317/0.15520, loss_mask_dice_7: 0.76966/0.46473, loss_mask_ce_8: 0.00824/0.06856, loss_mask_bce_8: 0.03338/0.15965, loss_mask_dice_8: 0.91464/0.47394, loss_mask_ce_9: 0.10705/0.12187, loss_mask_bce_9: 0.01715/0.18079, loss_mask_dice_9: 0.79395/0.54606] items per batch[4] items per second[0.25] total items[20400] mini batches[  5100] memory[2451] epoch remaining[0:00:23]
INFO:trainer.default_trainer:epochs[    13] optim steps[5200] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.00161/0.03401, loss_mask_bce_0: 0.06258/0.14908, loss_mask_dice_0: 0.22349/0.44876, loss_mask_ce_1: 0.00263/0.03519, loss_mask_bce_1: 0.06940/0.15151, loss_mask_dice_1: 0.20160/0.44788, loss_mask_ce_2: 0.00311/0.03501, loss_mask_bce_2: 0.07131/0.15073, loss_mask_dice_2: 0.20787/0.44484, loss_mask_ce_3: 0.00206/0.03506, loss_mask_bce_3: 0.06861/0.14969, loss_mask_dice_3: 0.21314/0.44373, loss_mask_ce_4: 0.00246/0.04485, loss_mask_bce_4: 0.07755/0.15181, loss_mask_dice_4: 0.27707/0.45405, loss_mask_ce_5: 0.00815/0.05219, loss_mask_bce_5: 0.06714/0.15240, loss_mask_dice_5: 0.22255/0.45312, loss_mask_ce_6: 0.00483/0.05192, loss_mask_bce_6: 0.07310/0.15219, loss_mask_dice_6: 0.28000/0.45154, loss_mask_ce_7: 0.01129/0.06083, loss_mask_bce_7: 0.06871/0.15476, loss_mask_dice_7: 0.20326/0.46410, loss_mask_ce_8: 0.01787/0.06849, loss_mask_bce_8: 0.06880/0.15913, loss_mask_dice_8: 0.27751/0.47323, loss_mask_ce_9: 0.04621/0.12213, loss_mask_bce_9: 0.07552/0.18027, loss_mask_dice_9: 0.21916/0.54499] items per batch[4] items per second[0.25] total items[20800] mini batches[  5200] memory[2452] epoch remaining[0:00:08]
INFO:trainer.default_trainer:Evaluation start ...
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0010 s/iter. Inference: 0.1138 s/iter. Eval: 0.3660 s/iter. Total: 0.4809 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0014 s/iter. Inference: 0.1137 s/iter. Eval: 0.3664 s/iter. Total: 0.4817 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0016 s/iter. Inference: 0.1137 s/iter. Eval: 0.3671 s/iter. Total: 0.4824 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0016 s/iter. Inference: 0.1136 s/iter. Eval: 0.3675 s/iter. Total: 0.4828 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0016 s/iter. Inference: 0.1135 s/iter. Eval: 0.3680 s/iter. Total: 0.4832 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0016 s/iter. Inference: 0.1134 s/iter. Eval: 0.3683 s/iter. Total: 0.4834 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:38.086351
INFO:trainer.default_trainer:PROGRESS: 93.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 14 training.
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.13s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.13 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.975
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.881
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.395
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.644
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.853
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.614
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.805
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.820
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.510
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.732
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 76.320 | 97.476 | 88.149 | 39.486 | 64.359 | 85.306 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 76.31990310761559, 'AP50': 97.47554187615184, 'AP75': 88.14925826692588, 'APs': 39.48603564376123, 'APm': 64.35890164495636, 'APl': 85.30647799245908})])}
INFO:trainer.default_trainer:This epoch takes 0:01:38.139246
INFO:trainer.default_trainer:PROGRESS: 93.33%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:trainer.default_trainer:Start epoch: 14 training.
INFO:trainer.default_trainer:epochs[    14] optim steps[5300] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.00037/0.03353, loss_mask_bce_0: 0.11175/0.14857, loss_mask_dice_0: 0.29385/0.44746, loss_mask_ce_1: 0.00073/0.03473, loss_mask_bce_1: 0.10893/0.15100, loss_mask_dice_1: 0.28172/0.44682, loss_mask_ce_2: 0.00057/0.03455, loss_mask_bce_2: 0.09934/0.15020, loss_mask_dice_2: 0.29097/0.44395, loss_mask_ce_3: 0.00050/0.03460, loss_mask_bce_3: 0.09560/0.14919, loss_mask_dice_3: 0.23228/0.44258, loss_mask_ce_4: 0.00031/0.04430, loss_mask_bce_4: 0.10210/0.15127, loss_mask_dice_4: 0.27406/0.45274, loss_mask_ce_5: 0.00055/0.05153, loss_mask_bce_5: 0.11771/0.15188, loss_mask_dice_5: 0.29967/0.45193, loss_mask_ce_6: 0.00035/0.05137, loss_mask_bce_6: 0.08803/0.15169, loss_mask_dice_6: 0.24860/0.45046, loss_mask_ce_7: 0.00061/0.06036, loss_mask_bce_7: 0.11109/0.15423, loss_mask_dice_7: 0.28862/0.46258, loss_mask_ce_8: 0.00125/0.06791, loss_mask_bce_8: 0.12264/0.15855, loss_mask_dice_8: 0.34711/0.47202, loss_mask_ce_9: 0.04745/0.12194, loss_mask_bce_9: 0.11692/0.17970, loss_mask_dice_9: 0.27581/0.54370] items per batch[4] items per second[0.07] total items[21200] mini batches[  5300] memory[2452] epoch remaining[0:00:53]
INFO:trainer.default_trainer:epochs[    14] optim steps[5400] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.00034/0.03334, loss_mask_bce_0: 0.04874/0.14866, loss_mask_dice_0: 0.19175/0.44669, loss_mask_ce_1: 0.00042/0.03441, loss_mask_bce_1: 0.05034/0.15154, loss_mask_dice_1: 0.25941/0.44572, loss_mask_ce_2: 0.00050/0.03431, loss_mask_bce_2: 0.04506/0.15077, loss_mask_dice_2: 0.18564/0.44317, loss_mask_ce_3: 0.00042/0.03434, loss_mask_bce_3: 0.04747/0.14976, loss_mask_dice_3: 0.25958/0.44174, loss_mask_ce_4: 0.00029/0.04395, loss_mask_bce_4: 0.04359/0.15151, loss_mask_dice_4: 0.20294/0.45194, loss_mask_ce_5: 0.00043/0.05122, loss_mask_bce_5: 0.04934/0.15179, loss_mask_dice_5: 0.22245/0.45149, loss_mask_ce_6: 0.00031/0.05094, loss_mask_bce_6: 0.03957/0.15164, loss_mask_dice_6: 0.19107/0.44950, loss_mask_ce_7: 0.00072/0.05978, loss_mask_bce_7: 0.04312/0.15416, loss_mask_dice_7: 0.19401/0.46171, loss_mask_ce_8: 0.00258/0.06760, loss_mask_bce_8: 0.04429/0.15844, loss_mask_dice_8: 0.22675/0.47113, loss_mask_ce_9: 0.04694/0.12137, loss_mask_bce_9: 0.05216/0.17955, loss_mask_dice_9: 0.22648/0.54211] items per batch[4] items per second[0.25] total items[21600] mini batches[  5400] memory[2452] epoch remaining[0:00:36]
INFO:trainer.default_trainer:epochs[    14] optim steps[5500] learning rate[default: 1.00000e-05] train loss[loss_mask_ce_0: 0.00091/0.03316, loss_mask_bce_0: 0.18469/0.14818, loss_mask_dice_0: 0.37887/0.44670, loss_mask_ce_1: 0.00215/0.03423, loss_mask_bce_1: 0.19777/0.15102, loss_mask_dice_1: 0.38713/0.44548, loss_mask_ce_2: 0.00318/0.03443, loss_mask_bce_2: 0.19902/0.15027, loss_mask_dice_2: 0.41132/0.44297, loss_mask_ce_3: 0.00278/0.03437, loss_mask_bce_3: 0.18723/0.14927, loss_mask_dice_3: 0.41055/0.44167, loss_mask_ce_4: 0.00531/0.04379, loss_mask_bce_4: 0.20207/0.15101, loss_mask_dice_4: 0.42592/0.45170, loss_mask_ce_5: 0.00247/0.05100, loss_mask_bce_5: 0.19718/0.15131, loss_mask_dice_5: 0.44490/0.45118, loss_mask_ce_6: 0.00073/0.05077, loss_mask_bce_6: 0.18737/0.15114, loss_mask_dice_6: 0.38227/0.44930, loss_mask_ce_7: 0.00035/0.05932, loss_mask_bce_7: 0.18462/0.15364, loss_mask_dice_7: 0.44957/0.46121, loss_mask_ce_8: 0.00118/0.06756, loss_mask_bce_8: 0.19534/0.15788, loss_mask_dice_8: 0.44345/0.47055, loss_mask_ce_9: 0.08773/0.12133, loss_mask_bce_9: 0.23760/0.17893, loss_mask_dice_9: 0.56158/0.54146] items per batch[4] items per second[0.25] total items[22000] mini batches[  5500] memory[2452] epoch remaining[0:00:20]
INFO:trainer.default_trainer:epochs[    14] optim steps[5600] learning rate[default: 1.00000e-05] train loss[loss_mask_ce_0: 0.00006/0.03282, loss_mask_bce_0: 0.08352/0.14792, loss_mask_dice_0: 0.15722/0.44574, loss_mask_ce_1: 0.00007/0.03390, loss_mask_bce_1: 0.08581/0.15067, loss_mask_dice_1: 0.16040/0.44438, loss_mask_ce_2: 0.00013/0.03409, loss_mask_bce_2: 0.08218/0.14990, loss_mask_dice_2: 0.16033/0.44187, loss_mask_ce_3: 0.00018/0.03406, loss_mask_bce_3: 0.08135/0.14895, loss_mask_dice_3: 0.15904/0.44048, loss_mask_ce_4: 0.00076/0.04345, loss_mask_bce_4: 0.09264/0.15065, loss_mask_dice_4: 0.16707/0.45065, loss_mask_ce_5: 0.00017/0.05048, loss_mask_bce_5: 0.09732/0.15094, loss_mask_dice_5: 0.17229/0.45005, loss_mask_ce_6: 0.00015/0.05039, loss_mask_bce_6: 0.08771/0.15080, loss_mask_dice_6: 0.16404/0.44844, loss_mask_ce_7: 0.00049/0.05887, loss_mask_bce_7: 0.08785/0.15322, loss_mask_dice_7: 0.16190/0.46017, loss_mask_ce_8: 0.00067/0.06714, loss_mask_bce_8: 0.09878/0.15745, loss_mask_dice_8: 0.16989/0.46962, loss_mask_ce_9: 0.08828/0.12132, loss_mask_bce_9: 0.11539/0.17852, loss_mask_dice_9: 0.17971/0.54013] items per batch[4] items per second[0.25] total items[22400] mini batches[  5600] memory[2452] epoch remaining[0:00:03]
WARNING:trainer.utils_trainer:Saving checkpoint...
WARNING:trainer.utils_trainer:Saving checkpoint...
WARNING:trainer.utils_trainer:Finished saving checkpoint and model to output/focall_unicl_lang_v1.yaml_conf~/run_6/00005625.
INFO:trainer.default_trainer:Evaluation start ...
WARNING:trainer.utils_trainer:Finished saving checkpoint and model to output/focall_unicl_lang_v1.yaml_conf~/run_6/00005625.
INFO:trainer.default_trainer:Evaluation start ...
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 11/75. Dataloading: 0.0010 s/iter. Inference: 0.1139 s/iter. Eval: 0.3668 s/iter. Total: 0.4817 s/iter. ETA=0:00:30
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 22/75. Dataloading: 0.0014 s/iter. Inference: 0.1138 s/iter. Eval: 0.3666 s/iter. Total: 0.4819 s/iter. ETA=0:00:25
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 33/75. Dataloading: 0.0015 s/iter. Inference: 0.1138 s/iter. Eval: 0.3663 s/iter. Total: 0.4816 s/iter. ETA=0:00:20
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 44/75. Dataloading: 0.0015 s/iter. Inference: 0.1136 s/iter. Eval: 0.3663 s/iter. Total: 0.4816 s/iter. ETA=0:00:14
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 55/75. Dataloading: 0.0015 s/iter. Inference: 0.1136 s/iter. Eval: 0.3658 s/iter. Total: 0.4810 s/iter. ETA=0:00:09
INFO:base_dir.pipeline.XDecoderPipeline:Task dolphin-val. Inference done 66/75. Dataloading: 0.0016 s/iter. Inference: 0.1135 s/iter. Eval: 0.3663 s/iter. Total: 0.4814 s/iter. ETA=0:00:04
INFO:trainer.default_trainer:This epoch takes 0:01:37.626121
INFO:trainer.default_trainer:PROGRESS: 100.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
INFO:detectron2.evaluation.coco_evaluation:Preparing results for COCO format ...
INFO:detectron2.evaluation.coco_evaluation:Saving results to output/coco_instances_results.json
INFO:detectron2.evaluation.coco_evaluation:Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *bbox*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.06 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
Loading and preparing results...
DONE (t=0.13s)
creating index...
index created!
INFO:detectron2.evaluation.fast_eval_api:Evaluate annotation type *segm*
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.evaluate() finished in 0.14 seconds.
INFO:detectron2.evaluation.fast_eval_api:Accumulating evaluation results...
INFO:detectron2.evaluation.fast_eval_api:COCOeval_opt.accumulate() finished in 0.01 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.765
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.873
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.373
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.644
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.804
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.515
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.731
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871
INFO:detectron2.evaluation.coco_evaluation:Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 76.505 | 97.723 | 87.344 | 37.345 | 64.372 | 85.576 |
INFO:trainer.default_trainer:{'dolphin-val/coco': OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 76.50506374732612, 'AP50': 97.72266622975003, 'AP75': 87.34415220235661, 'APs': 37.344866627539034, 'APm': 64.37164015108878, 'APl': 85.5760823813964})])}
INFO:trainer.default_trainer:This epoch takes 0:01:37.621170
INFO:trainer.default_trainer:PROGRESS: 100.00%
INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:  loss_mask_bce_0 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_1 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_2 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_3 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_4 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_5 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:  loss_mask_bce_6 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_7 ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_8 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  loss_mask_bce_9 ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   loss_mask_ce_0 ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_1 ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_2 ‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_3 ‚ñÅ‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_4 ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_5 ‚ñÜ‚ñÜ‚ñà‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_6 ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_7 ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_8 ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_mask_ce_9 ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_0 ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_1 ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_2 ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_3 ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_4 ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_5 ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_6 ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: loss_mask_dice_7 ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_8 ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb: loss_mask_dice_9 ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:  loss_mask_bce_0 0.08352
wandb:  loss_mask_bce_1 0.08581
wandb:  loss_mask_bce_2 0.08218
wandb:  loss_mask_bce_3 0.08135
wandb:  loss_mask_bce_4 0.09264
wandb:  loss_mask_bce_5 0.09732
wandb:  loss_mask_bce_6 0.08771
wandb:  loss_mask_bce_7 0.08785
wandb:  loss_mask_bce_8 0.09878
wandb:  loss_mask_bce_9 0.11539
wandb:   loss_mask_ce_0 6e-05
wandb:   loss_mask_ce_1 7e-05
wandb:   loss_mask_ce_2 0.00013
wandb:   loss_mask_ce_3 0.00018
wandb:   loss_mask_ce_4 0.00076
wandb:   loss_mask_ce_5 0.00017
wandb:   loss_mask_ce_6 0.00015
wandb:   loss_mask_ce_7 0.00049
wandb:   loss_mask_ce_8 0.00067
wandb:   loss_mask_ce_9 0.08828
wandb: loss_mask_dice_0 0.15722
wandb: loss_mask_dice_1 0.1604
wandb: loss_mask_dice_2 0.16033
wandb: loss_mask_dice_3 0.15904
wandb: loss_mask_dice_4 0.16707
wandb: loss_mask_dice_5 0.17229
wandb: loss_mask_dice_6 0.16404
wandb: loss_mask_dice_7 0.1619
wandb: loss_mask_dice_8 0.16989
wandb: loss_mask_dice_9 0.17971
wandb: 
wandb: üöÄ View run ndd20_lora_alpha-8__run_6 at: https://wandb.ai/david-rohrschneider/SEEM-adapter/runs/fjlrnf58
wandb: Ô∏è‚ö° View job at https://wandb.ai/david-rohrschneider/SEEM-adapter/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjIzNzM4Njc0Mw==/version_details/v10
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: output/focall_unicl_lang_v1.yaml_conf~/run_6/wandb/wandb/run-20240915_192618-fjlrnf58/logs
